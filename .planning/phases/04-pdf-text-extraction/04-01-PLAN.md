---
phase: 04-pdf-text-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/cer_scraper/config/settings.py
  - config/extraction.yaml
  - src/cer_scraper/db/models.py
  - src/cer_scraper/db/state.py
  - src/cer_scraper/extractor/__init__.py
autonomous: true
user_setup:
  - service: tesseract
    why: "OCR engine for scanned PDF pages"
    env_vars: []
    dashboard_config:
      - task: "Install Tesseract OCR"
        location: "Download from https://digi.bib.uni-mannheim.de/tesseract/ and run installer. Ensure added to PATH or set EXTRACTION_TESSERACT_CMD in .env"

must_haves:
  truths:
    - "pymupdf4llm, pdfplumber, python-frontmatter, pandas, and tabulate are installed as project dependencies"
    - "ExtractionSettings loads from config/extraction.yaml with env var overrides"
    - "Document model has extraction_status, extraction_method, extraction_error, extracted_text, char_count, and page_count columns"
    - "get_filings_for_extraction returns filings with status_downloaded=success and status_extracted!=success"
    - "extractor package directory exists as a proper Python package"
  artifacts:
    - path: "pyproject.toml"
      provides: "New PDF extraction dependencies"
      contains: "pymupdf4llm"
    - path: "src/cer_scraper/config/settings.py"
      provides: "ExtractionSettings class"
      contains: "class ExtractionSettings"
    - path: "config/extraction.yaml"
      provides: "Default extraction configuration"
      contains: "max_pages_for_extraction"
    - path: "src/cer_scraper/db/models.py"
      provides: "Extended Document model with extraction columns"
      contains: "extraction_status"
    - path: "src/cer_scraper/db/state.py"
      provides: "get_filings_for_extraction query function"
      contains: "def get_filings_for_extraction"
    - path: "src/cer_scraper/extractor/__init__.py"
      provides: "Extractor package marker"
  key_links:
    - from: "src/cer_scraper/config/settings.py"
      to: "config/extraction.yaml"
      via: "yaml_file path in SettingsConfigDict"
      pattern: "extraction\\.yaml"
    - from: "src/cer_scraper/db/state.py"
      to: "src/cer_scraper/db/models.py"
      via: "Filing.status_downloaded and Filing.status_extracted filters"
      pattern: "Filing\\.status_extracted"
---

<objective>
Install PDF extraction dependencies, add ExtractionSettings configuration, extend the Document model with extraction-tracking columns, and add the state store query for filings needing extraction.

Purpose: All downstream extraction code depends on these settings, model columns, and state queries. This plan establishes the foundation so Plans 02 and 03 can focus on extraction logic and orchestration.

Output: Updated pyproject.toml, new ExtractionSettings class, extended Document model, new get_filings_for_extraction function, and the extractor package directory.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-pdf-text-extraction/04-CONTEXT.md
@.planning/phases/04-pdf-text-extraction/04-RESEARCH.md
@src/cer_scraper/config/settings.py
@src/cer_scraper/db/models.py
@src/cer_scraper/db/state.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install extraction dependencies and create ExtractionSettings</name>
  <files>pyproject.toml, src/cer_scraper/config/settings.py, config/extraction.yaml</files>
  <action>
1. Run `uv add pymupdf4llm pdfplumber python-frontmatter pandas tabulate` to install dependencies. Note: the research mentions `pymupdf4llm[layout]` extra but pymupdf-layout may need separate install -- check if `pymupdf4llm` alone pulls in what's needed, and if not, also run `uv add pymupdf-layout`.

2. Add an `ExtractionSettings` class to `src/cer_scraper/config/settings.py` following the exact pattern of existing settings classes (ScraperSettings, PipelineSettings). It must:
   - Inherit from `BaseSettings`
   - Include `settings_customise_sources` classmethod for YAML activation (same pattern as other settings classes)
   - Fields:
     - `max_pages_for_extraction: int = 300` (skip PDFs with more pages)
     - `max_pages_for_ocr: int = 100` (skip OCR for very large docs)
     - `min_chars_per_page: int = 50` (quality threshold for text extraction)
     - `min_chars_per_page_ocr: int = 20` (looser threshold for OCR)
     - `garble_ratio_threshold: float = 0.05` (max non-printable char ratio for text)
     - `ocr_garble_ratio_threshold: float = 0.10` (max ratio for OCR)
     - `tesseract_cmd: str = "tesseract"` (path override if not on PATH)
     - `ocr_language: str = "eng"` (English only per user decision)
     - `ocr_dpi: int = 300` (rendering DPI for Tesseract)
     - `table_strategy: str = "lines_strict"` (pymupdf4llm table strategy)
   - `model_config` with `yaml_file=str(_CONFIG_DIR / "extraction.yaml")` and `env_prefix="EXTRACTION_"`

3. Create `config/extraction.yaml` with all defaults commented out (following the pattern of other YAML config files in the project -- check `config/scraper.yaml` or `config/pipeline.yaml` for style). Include a comment header explaining each setting group.
  </action>
  <verify>
Run `uv run python -c "from cer_scraper.config.settings import ExtractionSettings; s = ExtractionSettings(); print(s.max_pages_for_extraction, s.tesseract_cmd)"` and confirm it prints `300 tesseract`.

Run `uv run python -c "import pymupdf4llm; import pdfplumber; import frontmatter; import pandas; import tabulate; print('All imports OK')"` to verify all dependencies installed.
  </verify>
  <done>ExtractionSettings class exists, loads defaults correctly, and all five new dependencies are importable.</done>
</task>

<task type="auto">
  <name>Task 2: Extend Document model and add extraction state query</name>
  <files>src/cer_scraper/db/models.py, src/cer_scraper/db/state.py, src/cer_scraper/extractor/__init__.py</files>
  <action>
1. Add six new columns to the `Document` class in `src/cer_scraper/db/models.py`:
   - `extraction_status: Mapped[Optional[str]] = mapped_column(String(20), default=None)` -- values: None (not attempted), "success", "failed"
   - `extraction_method: Mapped[Optional[str]] = mapped_column(String(20), default=None)` -- values: "pymupdf4llm", "pdfplumber", "tesseract"
   - `extraction_error: Mapped[Optional[str]] = mapped_column(String(500), default=None)` -- reason for failure: "encrypted", "too_many_pages", "all_methods_failed"
   - `extracted_text: Mapped[Optional[str]] = mapped_column(Text, default=None)` -- full markdown content for SQL queries
   - `char_count: Mapped[Optional[int]] = mapped_column(default=None)` -- character count of extracted content
   - `page_count: Mapped[Optional[int]] = mapped_column(default=None)` -- number of pages in the PDF

   Place these after the existing `content_type` column, with a comment: `# Phase 4: extraction tracking`.

2. Add `get_filings_for_extraction` function to `src/cer_scraper/db/state.py`:
   - Query filings where `status_downloaded == "success"` AND `status_extracted != "success"` AND `retry_count < max_retries`
   - Use `selectinload(Filing.documents)` to eagerly load documents (same pattern as `get_filings_for_download`)
   - Docstring follows the exact style of `get_filings_for_download`

3. Create `src/cer_scraper/extractor/__init__.py` as an empty file (or with a module docstring only). The orchestrator will be added in Plan 03.

4. Since this project uses `Base.metadata.create_all()` (not Alembic), the new columns will be created automatically on next startup for a fresh database. For existing databases, the columns need to be added. Add a note in the docstring that existing databases may need `ALTER TABLE documents ADD COLUMN ...` statements, OR the user can delete and recreate the database (acceptable for development).
  </action>
  <verify>
Run `uv run python -c "from cer_scraper.db.models import Document; print([c.name for c in Document.__table__.columns])"` and confirm extraction_status, extraction_method, extraction_error, extracted_text, char_count, page_count appear in the column list.

Run `uv run python -c "from cer_scraper.db.state import get_filings_for_extraction; print('Query function imported OK')"` to confirm import.

Run `uv run python -c "import cer_scraper.extractor; print('Package imported OK')"` to confirm package exists.
  </verify>
  <done>Document model has 6 new extraction columns, get_filings_for_extraction function exists and is importable, and extractor package is created.</done>
</task>

</tasks>

<verification>
- `uv run python -c "from cer_scraper.config.settings import ExtractionSettings; s = ExtractionSettings(); print(vars(s))"` shows all extraction settings with correct defaults
- `uv run python -c "from cer_scraper.db.models import Document, Base; from cer_scraper.db.engine import get_engine; e = get_engine(); Base.metadata.create_all(e)"` creates the database with new columns without errors
- All new imports work without circular import issues
</verification>

<success_criteria>
1. Five new dependencies (pymupdf4llm, pdfplumber, python-frontmatter, pandas, tabulate) are in pyproject.toml and importable
2. ExtractionSettings loads from extraction.yaml with correct defaults
3. Document model includes 6 new extraction-tracking columns
4. get_filings_for_extraction returns correct filings based on download/extraction status
5. extractor package directory exists and is importable
</success_criteria>

<output>
After completion, create `.planning/phases/04-pdf-text-extraction/04-01-SUMMARY.md`
</output>
