---
phase: 04-pdf-text-extraction
plan: 03
type: execute
wave: 3
depends_on: ["04-01", "04-02"]
files_modified:
  - src/cer_scraper/extractor/markdown.py
  - src/cer_scraper/extractor/__init__.py
autonomous: true

must_haves:
  truths:
    - "Extracted markdown is written alongside the PDF with YAML frontmatter containing source_pdf, extraction_method, extraction_date, page_count, char_count"
    - "Extraction is idempotent: existing .md files with content are skipped on re-run"
    - "Document records are updated with extraction_status, extraction_method, extracted_text, char_count, and page_count on success"
    - "Failed documents are marked extraction_status='failed' with the error reason and pipeline continues"
    - "Filing status_extracted is set to 'success' when at least one document is successfully extracted"
    - "Per-filing error isolation: one filing failure does not block other filings"
    - "Each filing is committed independently"
  artifacts:
    - path: "src/cer_scraper/extractor/markdown.py"
      provides: "Markdown file writing with YAML frontmatter and idempotency check"
      exports: ["write_markdown_file", "should_extract"]
      min_lines: 25
    - path: "src/cer_scraper/extractor/__init__.py"
      provides: "Filing-level extraction orchestrator"
      exports: ["extract_filings", "ExtractionBatchResult"]
      min_lines: 60
  key_links:
    - from: "src/cer_scraper/extractor/__init__.py"
      to: "src/cer_scraper/extractor/service.py"
      via: "extract_document function call for each document"
      pattern: "extract_document"
    - from: "src/cer_scraper/extractor/__init__.py"
      to: "src/cer_scraper/extractor/markdown.py"
      via: "write_markdown_file and should_extract calls"
      pattern: "write_markdown_file|should_extract"
    - from: "src/cer_scraper/extractor/__init__.py"
      to: "src/cer_scraper/db/state.py"
      via: "get_filings_for_extraction and mark_step_complete calls"
      pattern: "get_filings_for_extraction|mark_step_complete"
    - from: "src/cer_scraper/extractor/__init__.py"
      to: "src/cer_scraper/db/models.py"
      via: "Document model attribute updates (extraction_status, extracted_text, etc.)"
      pattern: "doc\\.extraction_status|doc\\.extracted_text"
    - from: "src/cer_scraper/extractor/markdown.py"
      to: "python-frontmatter"
      via: "frontmatter.Post and frontmatter.dumps for YAML frontmatter"
      pattern: "frontmatter\\.Post|frontmatter\\.dumps"
---

<objective>
Create the markdown output writer and the filing-level extraction orchestrator that wires everything together: queries filings needing extraction, extracts each document using the tiered service, writes markdown files with frontmatter, updates database records, and tracks batch statistics.

Purpose: This plan completes Phase 4 by connecting the extraction engines (Plan 02) to the database and filesystem. It mirrors the downloader orchestrator pattern from Phase 3 for consistency.

Output: Markdown output module and filing-level orchestrator with public API `extract_filings()`.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-pdf-text-extraction/04-CONTEXT.md
@.planning/phases/04-pdf-text-extraction/04-RESEARCH.md
@.planning/phases/04-pdf-text-extraction/04-01-SUMMARY.md
@.planning/phases/04-pdf-text-extraction/04-02-SUMMARY.md
@src/cer_scraper/extractor/service.py
@src/cer_scraper/db/state.py
@src/cer_scraper/db/models.py
@src/cer_scraper/downloader/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create markdown output writer with frontmatter</name>
  <files>src/cer_scraper/extractor/markdown.py</files>
  <action>
Create `src/cer_scraper/extractor/markdown.py` with two functions:

**1. `should_extract(md_path: Path) -> bool`**
- Return False if `md_path.exists()` and `md_path.stat().st_size > 0` (skip if .md already exists with content)
- Return True otherwise
- Per user decision: "Skip extraction if .md file already exists and has content (no re-extract on re-run)"

**2. `write_markdown_file(md_path: Path, markdown_content: str, method: str, page_count: int, char_count: int, pdf_filename: str) -> None`**
- Use `python-frontmatter` library (`import frontmatter`)
- Create a `frontmatter.Post(markdown_content)` with metadata dict:
  - `source_pdf`: pdf_filename (just the filename, not full path)
  - `extraction_method`: method (string value like "pymupdf4llm")
  - `extraction_date`: `datetime.datetime.now(datetime.UTC).isoformat()` (use `import datetime` fully qualified to avoid Pydantic v2 shadowing bug from Phase 2 learnings)
  - `page_count`: page_count (int)
  - `char_count`: char_count (int)
- Ensure parent directory exists: `md_path.parent.mkdir(parents=True, exist_ok=True)`
- Write with `open(md_path, "w", encoding="utf-8")` and `frontmatter.dumps(post)`
- Log at INFO level: "Wrote extraction to {md_path.name} ({char_count} chars, {page_count} pages)"

Per user decision: "Extraction metadata stored in BOTH database AND YAML frontmatter in the .md file" -- the database update happens in the orchestrator, this module handles the file side.
  </action>
  <verify>
Run `uv run python -c "from cer_scraper.extractor.markdown import should_extract, write_markdown_file; print('Markdown module OK')"` to verify import.
  </verify>
  <done>write_markdown_file creates .md files with YAML frontmatter containing all five metadata fields. should_extract implements idempotency check.</done>
</task>

<task type="auto">
  <name>Task 2: Create filing-level extraction orchestrator</name>
  <files>src/cer_scraper/extractor/__init__.py</files>
  <action>
Replace the empty `src/cer_scraper/extractor/__init__.py` with the filing-level extraction orchestrator. Follow the EXACT pattern of `src/cer_scraper/downloader/__init__.py` (the download orchestrator) for consistency.

**Module docstring** explaining the orchestrator's role, public API.

**`__all__` = `["extract_filings", "ExtractionBatchResult"]`**

**`ExtractionBatchResult` dataclass:**
```python
@dataclass
class ExtractionBatchResult:
    filings_attempted: int = 0
    filings_succeeded: int = 0
    filings_failed: int = 0
    filings_skipped: int = 0
    total_docs_extracted: int = 0
    total_docs_failed: int = 0
    errors: list[str] = field(default_factory=list)
```

**`_extract_filing_documents(session, filing, settings) -> tuple[bool, str | None, int, int]`**

Per-filing extraction logic (mirrors `_download_filing` in the downloader):
1. Iterate over `filing.documents`
2. Skip documents where `doc.download_status != "success"` (not downloaded)
3. Build `pdf_path = Path(doc.local_path)` and `md_path = pdf_path.with_suffix(".md")`
4. Check idempotency: if `not should_extract(md_path)`, log skip, count as success, continue
5. Call `extract_document(pdf_path, settings)` from the extraction service
6. On success:
   - Call `write_markdown_file(md_path, result.markdown, result.method.value, result.page_count, result.char_count, pdf_path.name)`
   - Update Document record: `doc.extraction_status = "success"`, `doc.extraction_method = result.method.value`, `doc.extracted_text = result.markdown`, `doc.char_count = result.char_count`, `doc.page_count = result.page_count`
   - Increment success counter
7. On failure:
   - Update Document record: `doc.extraction_status = "failed"`, `doc.extraction_error = result.error`
   - Increment fail counter, log warning
8. Return `(has_any_success, error_summary, success_count, fail_count)`

NOTE: Unlike the downloader, extraction does NOT use all-or-nothing semantics. Per the user decision: "If all three methods fail: mark document as extraction_failed in DB, log a warning, and continue pipeline -- filing proceeds to email without text analysis." So individual document failures do NOT cause the entire filing to fail. A filing is considered "succeeded" if AT LEAST ONE document was successfully extracted.

**`extract_filings(session, extraction_settings) -> ExtractionBatchResult`**

Public API (mirrors `download_filings`):
1. Create batch result
2. Call `get_filings_for_extraction(session, max_retries)` -- use a sensible max_retries (3, from PipelineSettings or hardcoded)
3. If no filings, log "No filings pending extraction" and return
4. Log count of filings found
5. For each filing:
   - Increment filings_attempted
   - Log start: "Extracting filing {filing.filing_id} ({len(filing.documents)} documents)"
   - Call `_extract_filing_documents(session, filing, extraction_settings)`
   - On success: call `mark_step_complete(session, filing.filing_id, "extracted", "success")`, commit, increment filings_succeeded, update doc counters
   - On failure (no documents extracted): call `mark_step_complete(session, filing.filing_id, "extracted", "failed", error=error_msg)`, commit, increment filings_failed, append to errors
   - Wrap in try/except for unexpected errors: rollback, mark as failed, continue to next filing (per-filing error isolation)
6. Log batch summary
7. Return batch result

Import from:
- `cer_scraper.extractor.service`: `extract_document`
- `cer_scraper.extractor.markdown`: `write_markdown_file`, `should_extract`
- `cer_scraper.db.state`: `get_filings_for_extraction`, `mark_step_complete`
- `cer_scraper.db.models`: `Filing`
- `cer_scraper.config.settings`: `ExtractionSettings`
  </action>
  <verify>
Run `uv run python -c "from cer_scraper.extractor import extract_filings, ExtractionBatchResult; print('Orchestrator OK')"` to verify public API import.

Run `uv run python -c "
from cer_scraper.extractor import extract_filings, ExtractionBatchResult
from cer_scraper.db.engine import get_engine, get_session
from cer_scraper.db.models import Base
from cer_scraper.config.settings import ExtractionSettings
engine = get_engine()
Base.metadata.create_all(engine)
session = get_session(engine)
result = extract_filings(session, ExtractionSettings())
print(f'Attempted: {result.filings_attempted}, Succeeded: {result.filings_succeeded}')
session.close()
"` to verify the full wiring works (should report 0 filings if no downloaded filings exist).
  </verify>
  <done>Filing-level extraction orchestrator wires together extraction service, markdown writer, state queries, and database updates. Per-filing error isolation ensures one failure does not block others. Idempotent: skips already-extracted documents. Public API extract_filings() returns ExtractionBatchResult with full statistics.</done>
</task>

</tasks>

<verification>
- `from cer_scraper.extractor import extract_filings` imports without error
- `extract_filings(session, settings)` returns ExtractionBatchResult with correct field types
- If test filings with downloaded PDFs exist, running `extract_filings` produces .md files alongside PDFs with valid YAML frontmatter
- Document records in database have extraction_status, extraction_method, extracted_text, char_count, page_count populated after extraction
- Filing status_extracted is set to "success" or "failed" in the database
- Re-running extract_filings skips already-extracted documents (idempotency)
</verification>

<success_criteria>
1. Markdown files are written alongside PDFs with YAML frontmatter (source_pdf, extraction_method, extraction_date, page_count, char_count)
2. Idempotency works: existing .md files are not re-extracted
3. Document records updated in database with all extraction columns
4. Filing-level orchestrator processes all pending filings
5. Per-filing error isolation: one failure continues to next filing
6. Individual document failures within a filing do not fail the entire filing (unlike downloader's all-or-nothing)
7. Batch statistics accurately reflect extraction results
8. Full wiring: state query -> extraction service -> markdown writer -> database update -> status marking
</success_criteria>

<output>
After completion, create `.planning/phases/04-pdf-text-extraction/04-03-SUMMARY.md`
</output>
