---
phase: 05-core-llm-analysis
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/cer_scraper/analyzer/prompt.py
  - src/cer_scraper/analyzer/service.py
autonomous: true

must_haves:
  truths:
    - "Prompt template loads from disk and computes SHA-256 version hash"
    - "build_prompt fills all placeholders with filing data and document text"
    - "Claude CLI is invoked as subprocess with --output-format json, --max-turns 1, --tools empty, --no-session-persistence"
    - "CLI response is parsed at two levels: envelope JSON then analysis JSON inside result field"
    - "Markdown code fences are stripped from Claude's JSON response before parsing"
    - "Timeout kills the subprocess and raises a clear error"
    - "CLAUDECODE env var is stripped to prevent nested session errors"
    - "Windows uses CREATE_NEW_PROCESS_GROUP and encoding=utf-8"
  artifacts:
    - path: "src/cer_scraper/analyzer/prompt.py"
      provides: "load_prompt_template, build_prompt, get_json_schema_description functions"
    - path: "src/cer_scraper/analyzer/service.py"
      provides: "analyze_filing_text function returning AnalysisResult"
  key_links:
    - from: "src/cer_scraper/analyzer/service.py"
      to: "src/cer_scraper/analyzer/prompt.py"
      via: "imports load_prompt_template, build_prompt, get_json_schema_description"
      pattern: "from.*prompt import"
    - from: "src/cer_scraper/analyzer/service.py"
      to: "src/cer_scraper/analyzer/schemas.py"
      via: "AnalysisOutput.model_validate_json for response validation"
      pattern: "model_validate_json"
    - from: "src/cer_scraper/analyzer/service.py"
      to: "src/cer_scraper/analyzer/types.py"
      via: "returns AnalysisResult dataclass"
      pattern: "AnalysisResult"
---

<objective>
Build the core analysis service: prompt template management and Claude CLI subprocess invocation with robust JSON parsing.

Purpose: This is the heart of the LLM integration -- it loads the prompt template, fills variables with filing data, invokes `claude -p` as a subprocess, and parses the two-level JSON response (CLI envelope wrapping analysis JSON). Without this, no analysis can happen.

Output: prompt.py (template loading, variable substitution, version hashing) and service.py (CLI invocation, response parsing, error handling).
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-core-llm-analysis/05-RESEARCH.md
@.planning/phases/05-core-llm-analysis/05-CONTEXT.md
@.planning/phases/05-core-llm-analysis/05-01-SUMMARY.md  # Need types, schemas, settings

# Existing code to reference
@src/cer_scraper/analyzer/types.py      # AnalysisResult dataclass
@src/cer_scraper/analyzer/schemas.py    # AnalysisOutput Pydantic model
@src/cer_scraper/config/settings.py     # AnalysisSettings (model, timeout, template_path)
@config/prompts/filing_analysis.txt     # The prompt template
</context>

<tasks>

<task type="auto">
  <name>Task 1: Prompt template management</name>
  <files>src/cer_scraper/analyzer/prompt.py</files>
  <action>
Create `src/cer_scraper/analyzer/prompt.py` with three functions:

1. `load_prompt_template(template_path: Path) -> tuple[str, str]`:
   - Read the template file with `encoding="utf-8"`
   - Compute version hash: `hashlib.sha256(content.encode("utf-8")).hexdigest()[:12]`
   - Return `(content, version_hash)`
   - Raise `FileNotFoundError` with clear message if template missing

2. `get_json_schema_description() -> str`:
   - Return a multi-line string describing the exact JSON schema Claude should produce
   - Must match the AnalysisOutput Pydantic model fields exactly
   - Include the CER-specific taxonomy list for classification.primary_type
   - Include entity type options: "company", "facility", "location", "regulatory_reference"
   - Include role options: "applicant", "intervener", "regulator", "contractor", "operator", "landowner", "consultant", "other"
   - Format as a readable JSON example with comments (NOT as a JSON Schema spec -- keep it human-readable for the LLM)

3. `build_prompt(template: str, filing_id: str, filing_date: str, applicant: str, filing_type: str, document_text: str, num_documents: int, num_missing: int, json_schema_description: str) -> str`:
   - Use `template.format(...)` with all variables
   - Default "Unknown" for filing_date, applicant, filing_type if they are None or empty
   - Return the filled prompt string

Use `from __future__ import annotations` and module-level `logger = logging.getLogger(__name__)`.
  </action>
  <verify>
    Run: `python -c "from cer_scraper.analyzer.prompt import load_prompt_template, build_prompt, get_json_schema_description; from pathlib import Path; t, v = load_prompt_template(Path('config/prompts/filing_analysis.txt')); p = build_prompt(t, 'C12345', '2026-01-15', 'Test Corp', 'Application', 'Document text here', 1, 0, get_json_schema_description()); print(f'version={v}, prompt_len={len(p)}')"` -- should print version hash and prompt length.
  </verify>
  <done>
    Prompt template loads from disk with SHA-256 version hash. build_prompt fills all placeholders. get_json_schema_description returns a human-readable schema description matching AnalysisOutput fields exactly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Claude CLI invocation service</name>
  <files>src/cer_scraper/analyzer/service.py</files>
  <action>
Create `src/cer_scraper/analyzer/service.py` with these components:

1. **Code fence stripping** -- module-level regex and function:
   ```python
   _CODE_FENCE_RE = re.compile(r"^\s*```(?:json)?\s*\n?(.*?)\n?\s*```\s*$", re.DOTALL)

   def strip_code_fences(text: str) -> str:
       text = text.strip()
       match = _CODE_FENCE_RE.match(text)
       return match.group(1).strip() if match else text
   ```

2. **`_invoke_claude_cli(prompt_text: str, model: str, timeout: int) -> dict`** -- private function:
   - Build command: `["claude", "-p", "--output-format", "json", "--model", model, "--max-turns", "1", "--no-session-persistence", "--tools", ""]`
   - On Windows (`sys.platform == "win32"`): set `creationflags=subprocess.CREATE_NEW_PROCESS_GROUP`
   - Strip `CLAUDECODE` env var: `env = {k: v for k, v in os.environ.items() if k != "CLAUDECODE"}`
   - Use `subprocess.Popen` with `stdin=PIPE, stdout=PIPE, stderr=PIPE, text=True, encoding="utf-8"`
   - `proc.communicate(input=prompt_text, timeout=timeout)`
   - On `TimeoutExpired`: call `proc.kill()` then `proc.communicate()` to clean up, then re-raise
   - On non-zero exit: raise `RuntimeError` with stderr content
   - Parse stdout as JSON and return the envelope dict

3. **`analyze_filing_text(filing_id: str, filing_date: str, applicant: str, filing_type: str, document_text: str, num_documents: int, num_missing: int, settings: AnalysisSettings) -> AnalysisResult`** -- the main public function:
   - Import AnalysisSettings, AnalysisResult, AnalysisOutput, and prompt functions
   - Check `len(document_text.strip()) < settings.min_text_length`: return early with `AnalysisResult(success=False, error="insufficient_text")`
   - Load prompt template via `load_prompt_template(PROJECT_ROOT / settings.template_path)`
   - Build the prompt via `build_prompt(...)` with `get_json_schema_description()`
   - Record start time with `time.monotonic()`
   - Call `_invoke_claude_cli(prompt, settings.model, settings.timeout_seconds)` inside a try/except:
     - `subprocess.TimeoutExpired` -> return `AnalysisResult(success=False, error="timeout", needs_chunking=True)`
     - `RuntimeError` -> return `AnalysisResult(success=False, error=str(e))`
     - `json.JSONDecodeError` -> return `AnalysisResult(success=False, error="invalid_cli_json")`
   - Compute `processing_time = time.monotonic() - start`
   - Check `envelope.get("is_error")`: return failure with `envelope.get("result", "Unknown CLI error")`
   - Extract `raw_result = envelope["result"]`
   - Strip code fences from raw_result
   - Validate with `AnalysisOutput.model_validate_json(raw_result)` inside try/except for `ValidationError` and `json.JSONDecodeError`
   - On validation failure: return `AnalysisResult(success=False, error=f"validation_error: {e}", raw_response=raw_result)`
   - On success: build AnalysisResult with:
     - `success=True`
     - `analysis_json=validated_output.model_dump()` (dict form)
     - `raw_response=raw_result`
     - `model=settings.model`
     - `prompt_version=version_hash`
     - `processing_time_seconds=processing_time`
     - `cost_usd=envelope.get("total_cost_usd")`
     - Parse usage if present: `envelope.get("usage", {}).get("input_tokens")`, same for output_tokens -- fall back to None
     - `timestamp=datetime.datetime.now(datetime.timezone.utc).isoformat()`
   - Return the AnalysisResult

Use `from __future__ import annotations`. Import AnalysisSettings from config, AnalysisResult from types, AnalysisOutput from schemas, prompt functions from prompt module. Use PROJECT_ROOT from config.settings for resolving template_path.

Important: Do NOT import the `datetime` module at the field level -- use `import datetime` at the module top to avoid Pydantic v2 shadowing issues (per project learnings).
  </action>
  <verify>
    Run: `python -c "from cer_scraper.analyzer.service import analyze_filing_text, strip_code_fences; r = strip_code_fences('\`\`\`json\n{\"test\": 1}\n\`\`\`'); print(r); from cer_scraper.config.settings import AnalysisSettings; s = AnalysisSettings(); result = analyze_filing_text('C123', '2026-01-01', 'Test', 'Application', 'short', 1, 0, s); print(f'success={result.success}, error={result.error}')"` -- should strip code fences correctly, and return insufficient_text for short input.

    Run: `python -c "from cer_scraper.analyzer.service import strip_code_fences; assert strip_code_fences('\`\`\`json\n{\"a\":1}\n\`\`\`') == '{\"a\":1}'; assert strip_code_fences('{\"a\":1}') == '{\"a\":1}'; print('Code fence stripping OK')"` -- validates both fenced and unfenced input.
  </verify>
  <done>
    Claude CLI invoked as subprocess with all required flags (--output-format json, --max-turns 1, --tools "", --no-session-persistence). Two-level JSON parsing works (envelope then result). Code fences stripped. Timeout kills process. CLAUDECODE stripped from env. Windows flags set. analyze_filing_text returns AnalysisResult with full metadata (model, prompt version, processing time, cost, tokens, timestamp). Short text returns early with insufficient_text error.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from cer_scraper.analyzer.prompt import load_prompt_template; print('prompt OK')"` -- imports cleanly
2. `python -c "from cer_scraper.analyzer.service import analyze_filing_text; print('service OK')"` -- imports cleanly
3. Code fence stripping handles: ```json wrapper, plain JSON, and whitespace variations
4. analyze_filing_text returns insufficient_text for text shorter than min_text_length
5. No new dependencies introduced (all stdlib: subprocess, json, hashlib, re, time, os, sys, datetime)
</verification>

<success_criteria>
- Prompt loads from template file with version hash
- build_prompt fills all {variable} placeholders
- Claude CLI invoked with correct flags for single-turn text analysis
- Two-level JSON parsing (CLI envelope -> result -> analysis JSON)
- Code fences stripped before JSON parsing
- Timeout kills process and returns needs_chunking=True
- AnalysisResult contains full metadata (model, prompt_version, processing_time, cost, tokens, timestamp)
- Short text returns early without wasting an API call
</success_criteria>

<output>
After completion, create `.planning/phases/05-core-llm-analysis/05-02-SUMMARY.md`
</output>
