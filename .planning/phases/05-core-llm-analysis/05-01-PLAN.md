---
phase: 05-core-llm-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cer_scraper/analyzer/__init__.py  # empty, just marks package
  - src/cer_scraper/analyzer/types.py
  - src/cer_scraper/analyzer/schemas.py
  - src/cer_scraper/config/settings.py
  - config/analysis.yaml
  - config/prompts/filing_analysis.txt
  - src/cer_scraper/db/models.py
  - src/cer_scraper/db/state.py
autonomous: true

must_haves:
  truths:
    - "AnalysisSettings loads from config/analysis.yaml with env var overrides"
    - "Pydantic schemas validate analysis JSON with entities, relationships, classification, summary, and key_facts"
    - "Filing model has analysis_json column for storing analysis output"
    - "get_filings_for_analysis returns filings that are extracted but not analyzed"
    - "Prompt template file exists with {variables} placeholders for filing data"
  artifacts:
    - path: "src/cer_scraper/analyzer/types.py"
      provides: "AnalysisResult dataclass with success, analysis_json, metadata fields"
    - path: "src/cer_scraper/analyzer/schemas.py"
      provides: "EntityRef, Relationship, Classification, AnalysisOutput Pydantic models"
    - path: "config/analysis.yaml"
      provides: "Analysis configuration (model, timeout, min_text_length, template_path)"
    - path: "config/prompts/filing_analysis.txt"
      provides: "Prompt template with {filing_id}, {document_text}, etc. placeholders"
    - path: "src/cer_scraper/db/state.py"
      provides: "get_filings_for_analysis function"
  key_links:
    - from: "src/cer_scraper/config/settings.py"
      to: "config/analysis.yaml"
      via: "AnalysisSettings.yaml_file path"
      pattern: "analysis\\.yaml"
    - from: "src/cer_scraper/db/state.py"
      to: "src/cer_scraper/db/models.py"
      via: "Filing.status_extracted == success AND status_analyzed != success"
      pattern: "get_filings_for_analysis"
---

<objective>
Create the foundation for the LLM analysis module: configuration, Pydantic output schemas, shared types, prompt template, Filing model extension, and state query.

Purpose: All downstream analyzer code (CLI service, prompt builder, orchestrator) depends on these types, schemas, and configuration. Establishing them first avoids circular dependencies and ensures a clean contract between components.

Output: analyzer/ package skeleton, AnalysisSettings class, Pydantic schemas for analysis output validation, AnalysisResult dataclass, prompt template file, analysis_json column on Filing, get_filings_for_analysis state query.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-core-llm-analysis/05-RESEARCH.md
@.planning/phases/05-core-llm-analysis/05-CONTEXT.md

# Existing patterns to follow
@src/cer_scraper/extractor/types.py          # types.py pattern
@src/cer_scraper/config/settings.py          # Settings class pattern
@src/cer_scraper/db/models.py                # Filing model
@src/cer_scraper/db/state.py                 # State query pattern (get_filings_for_extraction)
@config/extraction.yaml                      # YAML config pattern
</context>

<tasks>

<task type="auto">
  <name>Task 1: Analysis config, schemas, types, and prompt template</name>
  <files>
    src/cer_scraper/analyzer/__init__.py
    src/cer_scraper/analyzer/types.py
    src/cer_scraper/analyzer/schemas.py
    src/cer_scraper/config/settings.py
    config/analysis.yaml
    config/prompts/filing_analysis.txt
  </files>
  <action>
1. Create `src/cer_scraper/analyzer/__init__.py` as an empty package marker (just a module docstring, no imports yet -- the orchestrator goes here in Plan 03).

2. Create `src/cer_scraper/analyzer/types.py` with an `AnalysisResult` dataclass:
   - `success: bool`
   - `analysis_json: dict | None = None` (the validated analysis output as dict)
   - `raw_response: str = ""` (raw text from Claude)
   - `model: str = ""` (model used, e.g. "sonnet")
   - `prompt_version: str = ""` (SHA-256 hash prefix of template)
   - `processing_time_seconds: float = 0.0`
   - `cost_usd: float | None = None`
   - `input_tokens: int | None = None`
   - `output_tokens: int | None = None`
   - `error: str | None = None`
   - `needs_chunking: bool = False` (flag for Phase 7 long-doc handling)
   - `timestamp: str = ""` (ISO 8601 timestamp of analysis completion)

3. Create `src/cer_scraper/analyzer/schemas.py` with Pydantic v2 models matching the locked decision output structure:
   - `EntityRef(BaseModel)`: name (str), type (str: "company", "facility", "location", "regulatory_reference"), role (str | None, e.g. "applicant", "intervener", "regulator", "contractor")
   - `Relationship(BaseModel)`: subject (str), predicate (str), object (str), context (str | None = None)
   - `Classification(BaseModel)`: primary_type (str), tags (list[str] = []), confidence (int, ge=0, le=100), justification (str)
   - `AnalysisOutput(BaseModel)`: summary (str), entities (list[EntityRef]), relationships (list[Relationship]), classification (Classification), key_facts (list[str])
   - Add a class docstring on AnalysisOutput listing the CER-specific taxonomy: Application, Order, Decision, Compliance Filing, Correspondence, Notice, Conditions Compliance, Financial Submission, Safety Report, Environmental Assessment.

4. Add `AnalysisSettings` to `src/cer_scraper/config/settings.py` following the exact same pattern as ExtractionSettings:
   - `model: str = "sonnet"` (Claude model alias)
   - `timeout_seconds: int = 300` (5 min default)
   - `min_text_length: int = 100` (skip analysis if combined text below this)
   - `template_path: str = "config/prompts/filing_analysis.txt"` (relative to PROJECT_ROOT)
   - model_config with `yaml_file=str(_CONFIG_DIR / "analysis.yaml")`, `env_prefix="ANALYSIS_"`
   - Same `settings_customise_sources` hook as other settings classes

5. Create `config/analysis.yaml` with commented-out defaults (same style as extraction.yaml):
   ```yaml
   # LLM analysis settings
   #
   # model: "sonnet"              # Claude model alias (sonnet, opus, haiku)
   # timeout_seconds: 300          # Max seconds to wait for Claude CLI response
   # min_text_length: 100          # Skip analysis if combined document text is shorter
   # template_path: "config/prompts/filing_analysis.txt"  # Prompt template location
   ```

6. Create `config/prompts/` directory and `config/prompts/filing_analysis.txt` with the analysis prompt template. The prompt must:
   - Set the role as a CER regulatory filing analyst
   - Instruct Claude to return ONLY a JSON object (no other text)
   - Include a clear JSON schema description with all required fields: summary, entities (with type and role), relationships, classification (with primary_type, tags, confidence 0-100, justification), key_facts
   - List the CER-specific taxonomy for classification
   - Use these placeholders: {filing_id}, {filing_date}, {applicant}, {filing_type}, {num_documents}, {num_missing}, {document_text}, {json_schema_description}
   - Include instructions for handling edge cases: missing metadata ("Unknown"), non-English content (analyze in the language present, provide English summary), short documents
   - Emphasize that entities should have roles (applicant, intervener, regulator, contractor, etc.) and that relationships should connect entities with predicates
  </action>
  <verify>
    Run `python -c "from cer_scraper.analyzer.types import AnalysisResult; from cer_scraper.analyzer.schemas import AnalysisOutput, EntityRef, Classification, Relationship; from cer_scraper.config.settings import AnalysisSettings; s = AnalysisSettings(); print(f'model={s.model}, timeout={s.timeout_seconds}')"` -- should print defaults without error.
    Run `python -c "from cer_scraper.analyzer.schemas import AnalysisOutput; import json; sample = {'summary': 'Test', 'entities': [{'name': 'TC Energy', 'type': 'company', 'role': 'applicant'}], 'relationships': [{'subject': 'TC Energy', 'predicate': 'applied for', 'object': 'export permit'}], 'classification': {'primary_type': 'Application', 'tags': ['export'], 'confidence': 85, 'justification': 'Test'}, 'key_facts': ['Fact 1']}; a = AnalysisOutput.model_validate(sample); print(a.classification.confidence)"` -- should print 85.
    Verify `config/prompts/filing_analysis.txt` exists and contains {filing_id}, {document_text} placeholders.
  </verify>
  <done>
    AnalysisSettings loads from YAML with env overrides. AnalysisOutput Pydantic model validates analysis JSON with all required fields (summary, entities with roles, relationships, classification with confidence 0-100, key_facts). AnalysisResult dataclass holds full metadata. Prompt template file exists with all required placeholders.
  </done>
</task>

<task type="auto">
  <name>Task 2: Filing model extension and state query for analysis</name>
  <files>
    src/cer_scraper/db/models.py
    src/cer_scraper/db/state.py
  </files>
  <action>
1. Add `analysis_json` column to the Filing model in `src/cer_scraper/db/models.py`:
   - `analysis_json: Mapped[Optional[str]] = mapped_column(Text, default=None)` -- stores the full analysis JSON as a serialized string
   - Place it after the `url` field and before the status fields, with a comment: `# Phase 5: LLM analysis output (JSON string)`
   - Note in the class docstring that Phase 5 adds this column (same pattern as Document's Phase 4 extraction columns)

2. Add `get_filings_for_analysis` function to `src/cer_scraper/db/state.py`:
   - Follow the exact pattern of `get_filings_for_extraction`
   - Query condition: `status_extracted == "success"` AND `status_analyzed != "success"` AND `retry_count < max_retries`
   - Use `selectinload(Filing.documents)` for eager loading (needed to assemble filing text)
   - Add the function to the module docstring list at the top
  </action>
  <verify>
    Run `python -c "from cer_scraper.db.models import Filing; print([c.name for c in Filing.__table__.columns if 'analysis' in c.name])"` -- should include 'analysis_json'.
    Run `python -c "from cer_scraper.db.state import get_filings_for_analysis; print('OK')"` -- should import without error.
  </verify>
  <done>
    Filing model has analysis_json Text column. get_filings_for_analysis returns extracted-but-not-analyzed filings with eagerly loaded documents, following the established state query pattern.
  </done>
</task>

</tasks>

<verification>
1. All imports resolve: `python -c "from cer_scraper.analyzer.types import AnalysisResult; from cer_scraper.analyzer.schemas import AnalysisOutput; from cer_scraper.config.settings import AnalysisSettings; from cer_scraper.db.state import get_filings_for_analysis; print('All imports OK')"`
2. AnalysisSettings defaults: model="sonnet", timeout_seconds=300, min_text_length=100
3. AnalysisOutput validates sample JSON with entities, relationships, classification (confidence 0-100), key_facts
4. Filing.analysis_json column exists in table definition
5. Prompt template file at config/prompts/filing_analysis.txt contains {filing_id} and {document_text} placeholders
6. config/analysis.yaml exists with commented defaults
</verification>

<success_criteria>
- AnalysisSettings class loads from analysis.yaml with ANALYSIS_ env prefix
- Pydantic schemas validate analysis output with all user-specified fields
- Filing model includes analysis_json column
- State query get_filings_for_analysis returns correct filings
- Prompt template has all required placeholders
- No new dependencies needed (all stdlib + existing pydantic)
</success_criteria>

<output>
After completion, create `.planning/phases/05-core-llm-analysis/05-01-SUMMARY.md`
</output>
