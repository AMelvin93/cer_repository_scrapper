---
phase: 02-regdocs-scraper
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - config/scraper.yaml
  - src/cer_scraper/config/settings.py
  - src/cer_scraper/scraper/__init__.py
  - src/cer_scraper/scraper/models.py
  - src/cer_scraper/scraper/rate_limiter.py
  - src/cer_scraper/scraper/robots.py
autonomous: true

must_haves:
  truths:
    - "ScraperSettings includes all Phase 2 config fields (delay_min/max, lookback_period, max_retries, backoff_base/max, discovery_retries, filing_type_include/exclude, applicant_filter, proceeding_filter)"
    - "scraper.yaml contains the new Phase 2 config fields with sensible defaults"
    - "ScrapedFiling and ScrapedDocument Pydantic models validate scraper output"
    - "Rate limiter produces randomized delays between configurable min and max seconds"
    - "robots.txt checker uses urllib.robotparser and allows scraping when robots.txt returns 404"
  artifacts:
    - path: "src/cer_scraper/scraper/__init__.py"
      provides: "Scraper package initialization"
    - path: "src/cer_scraper/scraper/models.py"
      provides: "ScrapedFiling and ScrapedDocument Pydantic models"
      contains: "class ScrapedFiling"
    - path: "src/cer_scraper/scraper/rate_limiter.py"
      provides: "Randomized delay function"
      contains: "def wait_between_requests"
    - path: "src/cer_scraper/scraper/robots.py"
      provides: "robots.txt compliance checker"
      contains: "def check_robots_allowed"
    - path: "config/scraper.yaml"
      provides: "Phase 2 scraper config values"
      contains: "lookback_period"
  key_links:
    - from: "src/cer_scraper/scraper/rate_limiter.py"
      to: "src/cer_scraper/config/settings.py"
      via: "Uses delay_min_seconds and delay_max_seconds from ScraperSettings"
    - from: "src/cer_scraper/scraper/robots.py"
      to: "src/cer_scraper/config/settings.py"
      via: "Uses base_url and user_agent from ScraperSettings"
---

<objective>
Install Phase 2 dependencies, extend scraper configuration with new fields, and create the scraper package foundation with Pydantic output models, a centralized rate limiter, and a robots.txt compliance checker.

Purpose: Establish the shared types, utilities, and configuration that the discovery, API client, DOM parser, and orchestrator modules all depend on.
Output: New `src/cer_scraper/scraper/` package with models.py, rate_limiter.py, robots.py; extended ScraperSettings; updated scraper.yaml and pyproject.toml.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-regdocs-scraper/02-RESEARCH.md
@src/cer_scraper/config/settings.py
@config/scraper.yaml
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies and extend ScraperSettings</name>
  <files>
    pyproject.toml
    src/cer_scraper/config/settings.py
    config/scraper.yaml
  </files>
  <action>
    1. Install Phase 2 dependencies using uv:
       ```
       uv add playwright httpx beautifulsoup4 lxml tenacity
       uv run playwright install chromium
       ```

    2. Extend the existing `ScraperSettings` class in `src/cer_scraper/config/settings.py` by adding these fields AFTER the existing ones (do NOT remove or rename existing fields):
       ```python
       # Phase 2: rate limiting
       delay_min_seconds: float = 1.0
       delay_max_seconds: float = 3.0

       # Phase 2: scraping scope
       lookback_period: str = "week"  # "day", "week", "month" -> maps to p=1, p=2, p=3

       # Phase 2: resilience
       max_retries: int = 3
       backoff_base: float = 2.0
       backoff_max: float = 30.0
       discovery_retries: int = 3

       # Phase 2: filtering
       filing_type_include: list[str] = []  # Empty = all types
       filing_type_exclude: list[str] = []
       applicant_filter: list[str] = []     # Empty = all applicants
       proceeding_filter: list[str] = []    # Empty = all proceedings
       ```

    3. Update `config/scraper.yaml` to include the new fields with their default values:
       ```yaml
       # Phase 2: rate limiting
       delay_min_seconds: 1.0
       delay_max_seconds: 3.0

       # Phase 2: scraping scope
       lookback_period: "week"

       # Phase 2: resilience
       max_retries: 3
       backoff_base: 2.0
       backoff_max: 30.0
       discovery_retries: 3

       # Phase 2: filtering (empty = no filter)
       filing_type_include: []
       filing_type_exclude: []
       applicant_filter: []
       proceeding_filter: []
       ```

    4. Verify that `ScraperSettings()` loads without errors:
       ```
       uv run python -c "from cer_scraper.config import ScraperSettings; s = ScraperSettings(); print(f'lookback={s.lookback_period}, delay={s.delay_min_seconds}-{s.delay_max_seconds}s')"
       ```
  </action>
  <verify>
    - `uv run python -c "from cer_scraper.config import ScraperSettings; s = ScraperSettings(); assert s.lookback_period == 'week'; assert s.delay_min_seconds == 1.0; assert s.delay_max_seconds == 3.0; assert s.max_retries == 3; assert s.discovery_retries == 3; assert s.filing_type_include == []; print('OK')"` prints OK
    - `uv run playwright install chromium` completes without error (may already be installed)
    - pyproject.toml contains all 5 new dependencies
  </verify>
  <done>ScraperSettings has all Phase 2 fields, scraper.yaml has matching defaults, all 5 dependencies installed.</done>
</task>

<task type="auto">
  <name>Task 2: Create scraper package with models, rate limiter, and robots checker</name>
  <files>
    src/cer_scraper/scraper/__init__.py
    src/cer_scraper/scraper/models.py
    src/cer_scraper/scraper/rate_limiter.py
    src/cer_scraper/scraper/robots.py
  </files>
  <action>
    1. Create `src/cer_scraper/scraper/` directory and `__init__.py`:
       - For now, just a docstring and empty `__all__` -- the public API (scrape_recent_filings) will be added in Plan 03.

    2. Create `src/cer_scraper/scraper/models.py` with two Pydantic models:

       **ScrapedDocument:**
       - `url: str` (required -- the document download URL)
       - `filename: Optional[str] = None`
       - `content_type: Optional[str] = None` (e.g., "application/pdf", "application/msword")

       **ScrapedFiling:**
       - `filing_id: str` (required -- must be non-empty)
       - `date: Optional[date] = None`
       - `applicant: Optional[str] = None`
       - `filing_type: Optional[str] = None`
       - `proceeding_number: Optional[str] = None`
       - `title: Optional[str] = None`
       - `url: Optional[str] = None`
       - `documents: list[ScrapedDocument] = Field(default_factory=list)`
       - Property `has_documents` -> bool (len(documents) > 0)
       - Validator on `filing_id`: must be non-empty string (use `@field_validator`)

    3. Create `src/cer_scraper/scraper/rate_limiter.py`:
       - Function `wait_between_requests(min_seconds: float = 1.0, max_seconds: float = 3.0) -> float`
       - Uses `random.uniform(min_seconds, max_seconds)` and `time.sleep()`
       - Logs the delay at DEBUG level: "Rate limit: waiting {delay:.1f}s"
       - Returns the actual delay applied (useful for testing)

    4. Create `src/cer_scraper/scraper/robots.py`:
       - Function `check_robots_allowed(base_url: str, target_path: str, user_agent: str) -> bool`
       - Uses `urllib.robotparser.RobotFileParser`
       - Sets URL to `{base_url}/robots.txt`
       - Calls `rp.read()` inside try/except -- if robots.txt is not found (404) or read fails, log a warning and return True (standard practice: no robots.txt = allowed)
       - If robots.txt is found, use `rp.can_fetch(user_agent, base_url + target_path)`
       - Check for `crawl_delay` directive and log it at INFO level if present
       - Returns True/False

    5. Verify all imports work:
       ```
       uv run python -c "
       from cer_scraper.scraper.models import ScrapedFiling, ScrapedDocument
       from cer_scraper.scraper.rate_limiter import wait_between_requests
       from cer_scraper.scraper.robots import check_robots_allowed
       print('All imports OK')
       "
       ```
  </action>
  <verify>
    - `uv run python -c "from cer_scraper.scraper.models import ScrapedFiling, ScrapedDocument; f = ScrapedFiling(filing_id='C12345', documents=[ScrapedDocument(url='http://example.com/doc.pdf')]); assert f.has_documents; print('Models OK')"` prints "Models OK"
    - `uv run python -c "from cer_scraper.scraper.models import ScrapedFiling; try: ScrapedFiling(filing_id=''); except Exception: print('Validation rejects empty filing_id OK')"` prints the validation message
    - `uv run python -c "from cer_scraper.scraper.rate_limiter import wait_between_requests; d = wait_between_requests(0.01, 0.02); assert 0.01 <= d <= 0.02; print('Rate limiter OK')"` prints "Rate limiter OK"
    - `uv run python -c "from cer_scraper.scraper.robots import check_robots_allowed; print('Robots import OK')"` succeeds
  </verify>
  <done>Scraper package exists with validated Pydantic models (ScrapedFiling rejects empty filing_id, ScrapedDocument validates URL), centralized rate limiter with randomized delays, and robots.txt compliance checker.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from cer_scraper.config import ScraperSettings; s = ScraperSettings(); print(vars(s))"` -- all Phase 2 fields present with correct defaults
2. `uv run python -c "from cer_scraper.scraper.models import ScrapedFiling, ScrapedDocument; print('OK')"` -- models importable
3. `uv run python -c "from cer_scraper.scraper.rate_limiter import wait_between_requests; print('OK')"` -- rate limiter importable
4. `uv run python -c "from cer_scraper.scraper.robots import check_robots_allowed; print('OK')"` -- robots checker importable
5. `uv run python main.py` -- existing application still starts correctly (no regressions)
</verification>

<success_criteria>
- pyproject.toml includes playwright, httpx, beautifulsoup4, lxml, tenacity as dependencies
- Chromium browser binary installed for Playwright
- ScraperSettings has all Phase 2 config fields with correct defaults
- config/scraper.yaml has all Phase 2 fields
- ScrapedFiling model validates filing_id (non-empty) and tracks documents list
- ScrapedDocument model captures URL, filename, content_type
- Rate limiter produces randomized delays and logs them
- robots.txt checker gracefully handles missing/broken robots.txt files
- Existing Phase 1 code (main.py, config, DB) still works without modification
</success_criteria>

<output>
After completion, create `.planning/phases/02-regdocs-scraper/02-01-SUMMARY.md`
</output>
