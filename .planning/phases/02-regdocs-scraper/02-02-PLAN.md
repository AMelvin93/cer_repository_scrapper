---
phase: 02-regdocs-scraper
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/cer_scraper/scraper/discovery.py
  - src/cer_scraper/scraper/api_client.py
autonomous: true

must_haves:
  truths:
    - "Discovery module captures all JSON/XML network responses during REGDOCS page navigation"
    - "page.on('response') listener is registered BEFORE page.goto() to avoid race condition"
    - "Discovery retries with different page navigations (p=1, p=2, p=3) before giving up"
    - "API client uses httpx sync API with tenacity retry (3 attempts, exponential backoff min=2 max=30)"
    - "API client transfers cookies from Playwright browser context to httpx for session continuity"
    - "User-Agent header is set to the configured descriptive string (not a browser UA)"
  artifacts:
    - path: "src/cer_scraper/scraper/discovery.py"
      provides: "Playwright network interception and API endpoint discovery"
      contains: "def discover_api_endpoints"
    - path: "src/cer_scraper/scraper/api_client.py"
      provides: "httpx-based API client with retry logic"
      contains: "def fetch_filings_from_api"
  key_links:
    - from: "src/cer_scraper/scraper/discovery.py"
      to: "playwright.sync_api"
      via: "sync_playwright context manager, page.on('response')"
      pattern: "page\\.on\\(\"response\""
    - from: "src/cer_scraper/scraper/api_client.py"
      to: "tenacity"
      via: "@retry decorator with wait_random_exponential"
      pattern: "@retry"
    - from: "src/cer_scraper/scraper/api_client.py"
      to: "src/cer_scraper/scraper/models.py"
      via: "Returns list[ScrapedFiling] from API response parsing"
    - from: "src/cer_scraper/scraper/discovery.py"
      to: "src/cer_scraper/scraper/rate_limiter.py"
      via: "Calls wait_between_requests() between page navigations"
---

<objective>
Build the Playwright network interception module that discovers REGDOCS API endpoints at runtime, and the httpx-based API client that uses discovered endpoints to fetch filing metadata with retry logic.

Purpose: Implement the primary (API-first) scraping strategy. This is the higher-value path -- if REGDOCS exposes usable API endpoints, the API client provides more reliable structured data than DOM parsing.
Output: `discovery.py` (Playwright interception) and `api_client.py` (httpx with tenacity retry).
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-regdocs-scraper/02-RESEARCH.md
@.planning/phases/02-regdocs-scraper/02-01-SUMMARY.md
@src/cer_scraper/config/settings.py
@src/cer_scraper/scraper/models.py
@src/cer_scraper/scraper/rate_limiter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Playwright network interception discovery module</name>
  <files>src/cer_scraper/scraper/discovery.py</files>
  <action>
    Create `src/cer_scraper/scraper/discovery.py` implementing API endpoint discovery via Playwright network interception.

    **Core data structure -- DiscoveredEndpoint:**
    A dataclass or Pydantic model capturing a discovered API endpoint:
    - `url: str` -- the full endpoint URL
    - `method: str` -- HTTP method (GET, POST)
    - `status_code: int`
    - `content_type: str`
    - `body: Any` -- parsed JSON/XML response body
    - `has_filing_data: bool` -- heuristic: does this look like filing data?

    **Main function -- `discover_api_endpoints(settings: ScraperSettings) -> DiscoveryResult`:**

    `DiscoveryResult` is a dataclass containing:
    - `endpoints: list[DiscoveredEndpoint]` -- all captured API-like responses
    - `filing_endpoints: list[DiscoveredEndpoint]` -- endpoints that appear to contain filing data
    - `cookies: dict[str, str]` -- cookies from the Playwright browser context (for API client)
    - `rendered_html: str` -- the last page's rendered HTML (for DOM parser fallback)
    - `success: bool` -- True if at least one filing endpoint was found

    **Implementation details:**

    1. Use `sync_playwright()` context manager (sync API, per user decision / research recommendation -- avoids Windows async issues).

    2. Launch Chromium headless with configured `user_agent`.

    3. Register `page.on("response", handle_response)` callback BEFORE any `page.goto()` -- critical to avoid race condition (Pitfall 1 from research).

    4. The `handle_response` callback should:
       - Check content-type for "json" or "xml"
       - Try to parse body as JSON
       - Log captured endpoint at INFO level
       - Classify whether the response looks like filing data using heuristics:
         - Contains list/array of items
         - Items have fields resembling filing metadata (id, date, name, title, type, etc.)
         - Response URL contains patterns like "search", "filing", "document", "recent"
       - Append to captured_responses list

    5. Navigate to REGDOCS Recent Filings page. The lookback_period config maps to URL parameter:
       - "day" -> p=1
       - "week" -> p=2
       - "month" -> p=3

    6. Wait for `networkidle` state with 30-second timeout.

    7. If no filing endpoints found after first navigation, retry with different page navigations (per user decision: retry 2-3 times with different pages). Use the configured `discovery_retries` count. Between retries:
       - Call `wait_between_requests()` for polite delay
       - Try different lookback periods (if first was p=2, try p=1 then p=3)
       - Log each retry attempt

    8. After discovery completes, extract cookies from browser context:
       ```python
       cookies = {c["name"]: c["value"] for c in context.cookies()}
       ```

    9. Capture the rendered HTML from the last visited page: `page.content()`

    10. Close browser and return DiscoveryResult.

    **Error handling:**
    - Wrap browser launch in try/except. If Playwright browser is not installed, log a clear error message: "Chromium browser not installed. Run: uv run playwright install chromium"
    - Set reasonable timeouts: 30s for page load, 60s for overall discovery
    - If all retries fail to find filing endpoints, return DiscoveryResult with success=False and the rendered HTML (for DOM fallback)

    **Logging:**
    - Use `logging.getLogger(__name__)` consistently
    - INFO: discovery start/end, endpoints found count, retry attempts
    - DEBUG: each captured response URL and content-type
    - WARNING: no filing endpoints found, browser not installed
  </action>
  <verify>
    - `uv run python -c "from cer_scraper.scraper.discovery import discover_api_endpoints, DiscoveryResult; print('Import OK')"` succeeds
    - `uv run python -c "from cer_scraper.scraper.discovery import DiscoveredEndpoint; e = DiscoveredEndpoint(url='http://test', method='GET', status_code=200, content_type='application/json', body={}, has_filing_data=False); print(e)"` succeeds
    - Code inspection: `page.on("response", ...)` appears BEFORE `page.goto()`
    - Code inspection: sync_playwright used (not async_playwright)
    - Code inspection: cookies extracted from browser context
    - Code inspection: rendered_html captured from page.content()
  </verify>
  <done>Discovery module uses Playwright sync API to intercept all JSON/XML responses during REGDOCS navigation, classifies filing-like endpoints, extracts cookies and rendered HTML, and retries with different page navigations per user decision.</done>
</task>

<task type="auto">
  <name>Task 2: Create httpx API client with tenacity retry</name>
  <files>src/cer_scraper/scraper/api_client.py</files>
  <action>
    Create `src/cer_scraper/scraper/api_client.py` implementing direct API calls to discovered endpoints.

    **Main function -- `fetch_filings_from_api(endpoints: list[DiscoveredEndpoint], cookies: dict[str, str], settings: ScraperSettings) -> list[ScrapedFiling]`:**

    1. Create an httpx.Client with:
       - `headers={"User-Agent": settings.user_agent, "Accept": "application/json"}`
       - `cookies=cookies` (transferred from Playwright discovery)
       - `timeout=httpx.Timeout(30.0)`
       - `follow_redirects=True`

    2. For each filing endpoint in `endpoints`:
       - Call `_fetch_endpoint(client, endpoint.url)` -- the retry-wrapped function
       - Parse the response into `ScrapedFiling` objects using `_parse_api_response()`
       - Call `wait_between_requests(settings.delay_min_seconds, settings.delay_max_seconds)` between requests

    3. Close the httpx client after all endpoints are processed.

    **Retry function -- `_fetch_endpoint(client: httpx.Client, url: str) -> dict`:**
    - Decorated with tenacity `@retry`:
      ```python
      @retry(
          stop=stop_after_attempt(3),
          wait=wait_random_exponential(multiplier=1, min=2, max=30),
          before_sleep=before_sleep_log(logger, logging.WARNING),
          retry=retry_if_exception_type((httpx.HTTPStatusError, httpx.TimeoutException, httpx.ConnectError)),
      )
      ```
    - Calls `client.get(url)`, then `response.raise_for_status()`, then `response.json()`
    - Returns the parsed JSON dict

    **Parse function -- `_parse_api_response(url: str, data: Any) -> list[ScrapedFiling]`:**
    - This function must be resilient to unknown JSON structures (since the API format is discovered at runtime).
    - Strategy: look for list-like structures in the response. If `data` is a list, iterate items. If `data` is a dict, look for values that are lists.
    - For each item that looks like a filing, try to extract:
      - `filing_id`: look for keys like "id", "filing_id", "filingId", "ID", "NodeID"
      - `date`: look for keys like "date", "filing_date", "filingDate", "OTCreateDate", "CreateDate"
      - `applicant`: look for keys like "applicant", "company", "submitter", "Name", "OTName"
      - `filing_type`: look for keys like "type", "filing_type", "filingType", "SubType"
      - `proceeding_number`: look for keys like "proceeding", "proceeding_number", "ProceedingNumber"
      - `title`: look for keys like "title", "name", "Name", "OTName"
      - `url`: construct from filing_id if not directly available: `{base_url}/Item/Filing/{filing_id}`
      - `documents`: look for document URL patterns (keys containing "url", "link", "document", "pdf")
    - Use case-insensitive key matching for robustness
    - Log at DEBUG level what fields were extracted from each item
    - Skip items that have no identifiable filing_id (minimum required field)
    - Create ScrapedFiling objects, letting Pydantic validation catch invalid data
    - Wrap each item parsing in try/except to handle malformed data without crashing
    - Log WARNING for items that fail parsing

    **Error handling:**
    - If httpx gets a 401/403, log a specific warning: "API endpoint may require authentication. Cookies may have expired."
    - If all endpoints return errors, return empty list (let orchestrator fall back to DOM)
    - Never raise -- return empty list on complete failure

    **Logging:**
    - INFO: API client start, endpoints to query, total filings found
    - DEBUG: each endpoint URL, response status, items parsed
    - WARNING: auth errors, parse failures, empty responses
  </action>
  <verify>
    - `uv run python -c "from cer_scraper.scraper.api_client import fetch_filings_from_api; print('Import OK')"` succeeds
    - Code inspection: tenacity @retry decorator present with stop_after_attempt(3) and wait_random_exponential
    - Code inspection: httpx.Client created with user_agent, cookies, timeout
    - Code inspection: rate_limiter.wait_between_requests() called between endpoint fetches
    - Code inspection: _parse_api_response handles unknown JSON structures with resilient key matching
    - Code inspection: function returns list[ScrapedFiling], never raises on API errors
  </verify>
  <done>API client fetches filing data from discovered endpoints using httpx with Playwright cookies, retries HTTP errors with exponential backoff via tenacity, and parses unknown JSON structures into ScrapedFiling models with resilient field extraction.</done>
</task>

</tasks>

<verification>
1. Both modules import without errors
2. discovery.py registers page.on("response") before page.goto()
3. discovery.py uses sync_playwright (not async)
4. discovery.py extracts cookies and rendered HTML for fallback
5. api_client.py has tenacity @retry with configured exponential backoff
6. api_client.py transfers cookies from Playwright to httpx
7. api_client.py calls wait_between_requests() for polite delays
8. api_client.py returns list[ScrapedFiling] (never raises on errors)
</verification>

<success_criteria>
- Discovery module navigates REGDOCS, captures JSON/XML responses, classifies filing endpoints, and exports cookies + rendered HTML
- API client queries discovered endpoints with proper retry logic, parses unknown JSON into ScrapedFiling models
- Both modules use sync APIs only (no async -- Windows compatibility per research)
- Rate limiting applied between requests
- Descriptive User-Agent header used in all requests
- Errors are logged and handled gracefully (no crashes)
</success_criteria>

<output>
After completion, create `.planning/phases/02-regdocs-scraper/02-02-SUMMARY.md`
</output>
