---
phase: 02-regdocs-scraper
plan: 03
type: execute
wave: 3
depends_on: ["02-01", "02-02"]
files_modified:
  - src/cer_scraper/scraper/dom_parser.py
  - src/cer_scraper/scraper/__init__.py
autonomous: true

must_haves:
  truths:
    - "DOM parser extracts filing metadata from rendered REGDOCS HTML using BeautifulSoup with lxml"
    - "DOM parser produces the same ScrapedFiling output model as the API client"
    - "Scraper orchestrator tries API discovery first, falls back to DOM parsing on failure"
    - "Orchestrator skips filings that already exist in the state store (deduplication)"
    - "Orchestrator skips filings with no document URLs (per user decision)"
    - "Orchestrator applies filing type, applicant, and proceeding number filters from config"
    - "Orchestrator logs a WARNING when zero filings found for 3+ consecutive runs"
    - "Orchestrator validates scraped data (expected fields present, reasonable values)"
    - "Orchestrator respects robots.txt before scraping"
    - "Rate-limited delays applied between all network requests"
  artifacts:
    - path: "src/cer_scraper/scraper/dom_parser.py"
      provides: "BeautifulSoup DOM parsing fallback"
      contains: "def parse_filings_from_html"
    - path: "src/cer_scraper/scraper/__init__.py"
      provides: "Public API: scrape_recent_filings()"
      contains: "def scrape_recent_filings"
  key_links:
    - from: "src/cer_scraper/scraper/__init__.py"
      to: "src/cer_scraper/scraper/discovery.py"
      via: "Calls discover_api_endpoints() as primary strategy"
      pattern: "discover_api_endpoints"
    - from: "src/cer_scraper/scraper/__init__.py"
      to: "src/cer_scraper/scraper/api_client.py"
      via: "Calls fetch_filings_from_api() with discovered endpoints"
      pattern: "fetch_filings_from_api"
    - from: "src/cer_scraper/scraper/__init__.py"
      to: "src/cer_scraper/scraper/dom_parser.py"
      via: "Falls back to parse_filings_from_html() when API discovery fails"
      pattern: "parse_filings_from_html"
    - from: "src/cer_scraper/scraper/__init__.py"
      to: "src/cer_scraper/db/state.py"
      via: "Calls filing_exists() for deduplication, create_filing() for persistence"
      pattern: "filing_exists|create_filing"
    - from: "src/cer_scraper/scraper/__init__.py"
      to: "src/cer_scraper/scraper/robots.py"
      via: "Calls check_robots_allowed() before scraping"
      pattern: "check_robots_allowed"
    - from: "src/cer_scraper/scraper/__init__.py"
      to: "src/cer_scraper/db/models.py"
      via: "Queries RunHistory for consecutive zero-filing detection"
      pattern: "RunHistory"
---

<objective>
Build the BeautifulSoup DOM parsing fallback and the scraper orchestrator that ties all modules together -- API discovery, API client, DOM parser, filtering, deduplication, persistence, validation, and zero-filing tracking.

Purpose: Complete the Phase 2 scraper by providing the DOM fallback strategy and the public `scrape_recent_filings()` function that the pipeline will call in Phase 9.
Output: `dom_parser.py` (BeautifulSoup fallback) and updated `scraper/__init__.py` (orchestrator with public API).
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-regdocs-scraper/02-RESEARCH.md
@.planning/phases/02-regdocs-scraper/02-01-SUMMARY.md
@.planning/phases/02-regdocs-scraper/02-02-SUMMARY.md
@src/cer_scraper/scraper/models.py
@src/cer_scraper/scraper/discovery.py
@src/cer_scraper/scraper/api_client.py
@src/cer_scraper/scraper/rate_limiter.py
@src/cer_scraper/scraper/robots.py
@src/cer_scraper/db/state.py
@src/cer_scraper/db/models.py
@src/cer_scraper/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BeautifulSoup DOM parsing fallback</name>
  <files>src/cer_scraper/scraper/dom_parser.py</files>
  <action>
    Create `src/cer_scraper/scraper/dom_parser.py` implementing the DOM parsing fallback strategy.

    **Main function -- `parse_filings_from_html(html: str, base_url: str) -> list[ScrapedFiling]`:**

    Since the exact HTML structure of REGDOCS is unknown until runtime (and may change over time), the DOM parser must use a multi-strategy approach:

    1. Parse with BeautifulSoup using lxml backend: `BeautifulSoup(html, "lxml")`

    2. **Strategy 1 -- Table-based layout:** Look for `<table>` elements containing filing rows. REGDOCS typically renders search results in tables.
       - Find tables that have header rows with keywords: "Filing", "Date", "Applicant", "Type", "Proceeding"
       - For each data row, map cells to filing fields based on header position
       - Extract document links from anchor tags within rows

    3. **Strategy 2 -- Link-based extraction:** If no filing tables found, look for links matching REGDOCS URL patterns:
       - Filing links: `/Item/Filing/{id}` or `/Item/View/{id}`
       - Extract filing_id from URL path
       - Look at surrounding DOM context for metadata (parent elements, sibling text)

    4. **Strategy 3 -- Data attribute extraction:** Look for elements with data attributes:
       - `[data-filing-id]`, `[data-id]`, `[data-nodeid]`
       - Extract metadata from data attributes and inner text

    5. For each filing found:
       - Extract filing_id (required -- skip if not found)
       - Extract date (try multiple date formats: YYYY-MM-DD, MM/DD/YYYY, Month DD YYYY)
       - Extract applicant from text content
       - Extract filing type
       - Extract proceeding number
       - Extract title
       - Construct filing URL: `{base_url}/Item/Filing/{filing_id}`
       - Extract all document links:
         - Find all `<a>` tags linking to document URLs (containing "/Item/View/", ".pdf", ".doc", ".xls")
         - Create ScrapedDocument for each with URL, filename (from link text or URL), and inferred content_type

    6. Deduplicate extracted filings by filing_id (same filing may appear in multiple strategies).

    **Helper functions:**
    - `_extract_date(text: str) -> Optional[date]` -- Try multiple date formats, return None on failure
    - `_infer_content_type(url: str) -> Optional[str]` -- Map file extensions to MIME types (.pdf -> application/pdf, .doc -> application/msword, .xlsx -> application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, etc.)
    - `_clean_text(text: str) -> str` -- Strip whitespace, collapse multiple spaces, remove non-breaking spaces

    **Logging:**
    - INFO: DOM parsing start, strategy used, number of filings found
    - DEBUG: each filing extracted with its fields
    - WARNING: no filings found by any strategy, failed date parsing

    **Important:** CSS selectors are inherently fragile. Log clear warnings when selectors match zero elements so that site structure changes are quickly identified. This satisfies the user decision for "validation checks after each scrape" for the DOM path.
  </action>
  <verify>
    - `uv run python -c "from cer_scraper.scraper.dom_parser import parse_filings_from_html; print('Import OK')"` succeeds
    - `uv run python -c "
    from cer_scraper.scraper.dom_parser import parse_filings_from_html
    # Test with minimal HTML containing a filing link
    html = '<html><body><a href=\"/REGDOCS/Item/Filing/C12345\">Test Filing</a><a href=\"/REGDOCS/Item/View/4642764\">Document.pdf</a></body></html>'
    results = parse_filings_from_html(html, 'https://apps.cer-rec.gc.ca/REGDOCS')
    print(f'Found {len(results)} filings')
    "` -- should parse at least the filing link
    - Code inspection: Multiple parsing strategies attempted (table, link, data-attribute)
    - Code inspection: ScrapedFiling model used for output (same as API client)
  </verify>
  <done>DOM parser extracts filing metadata from rendered HTML using multiple strategies (table, link, data-attribute), produces ScrapedFiling output models identical to the API path, and logs clear warnings when no filings found.</done>
</task>

<task type="auto">
  <name>Task 2: Create scraper orchestrator with filtering, dedup, and persistence</name>
  <files>src/cer_scraper/scraper/__init__.py</files>
  <action>
    Update `src/cer_scraper/scraper/__init__.py` to implement the public scraper API.

    **Main function -- `scrape_recent_filings(session: Session, settings: ScraperSettings) -> ScrapeResult`:**

    `ScrapeResult` is a dataclass containing:
    - `total_found: int` -- total filings scraped (before filtering/dedup)
    - `new_filings: int` -- filings persisted to DB (after filtering/dedup)
    - `skipped_existing: int` -- filings already in DB
    - `skipped_no_documents: int` -- filings with no document URLs
    - `skipped_filtered: int` -- filings excluded by config filters
    - `strategy_used: str` -- "api" or "dom" (which strategy produced the data)
    - `errors: list[str]` -- any non-fatal errors encountered

    **Orchestration flow:**

    1. **robots.txt check:** Call `check_robots_allowed(settings.base_url, settings.recent_filings_path, settings.user_agent)`. If not allowed, log ERROR and return early with zero results.

    2. **API discovery (primary strategy):**
       - Call `discover_api_endpoints(settings)` to get DiscoveryResult
       - If discovery.success is True:
         - Call `fetch_filings_from_api(discovery.filing_endpoints, discovery.cookies, settings)`
         - If filings returned, set strategy_used = "api"

    3. **DOM parsing (fallback strategy):**
       - If API discovery failed OR API client returned zero filings:
         - If `discovery.rendered_html` is available, use it
         - Otherwise, launch a quick Playwright session to get rendered HTML (navigate, wait for networkidle, get page.content())
         - Call `parse_filings_from_html(html, settings.base_url)`
         - Set strategy_used = "dom"

    4. **Validate scraped data:**
       - For each ScrapedFiling, check:
         - filing_id is non-empty (Pydantic should already enforce this)
         - At least one document URL present (has_documents property)
       - Log validation failures at WARNING level with specific details
       - Count validation failures in errors list

    5. **Filter filings by config:**
       - If `settings.filing_type_include` is non-empty, keep only filings whose filing_type matches (case-insensitive)
       - If `settings.filing_type_exclude` is non-empty, remove filings whose filing_type matches
       - If `settings.applicant_filter` is non-empty, keep only filings whose applicant contains any filter string (case-insensitive substring match)
       - If `settings.proceeding_filter` is non-empty, keep only filings whose proceeding_number matches
       - Filings with None/empty filing_type are NOT filtered out by type filters (they pass through -- LLM may classify them later)
       - Log each filter application and how many filings it removed

    6. **Skip filings with no documents** (per user decision):
       - Remove filings where `has_documents` is False
       - Log count of skipped filings at INFO level

    7. **Deduplicate against state store:**
       - For each remaining filing, call `filing_exists(session, filing.filing_id)`
       - Skip existing filings, count them

    8. **Persist new filings:**
       - For each new filing, call `create_filing(session, ...)` with scraped metadata
       - Use placeholders for missing optional fields: applicant="Unknown" if None, etc.
       - Create Document records for each document URL
       - Commit after each filing (per Phase 1 pattern)

    9. **Zero-filing consecutive run tracking:**
       - After persistence, check if new_filings == 0
       - Query RunHistory for the last 3 runs (configurable threshold)
       - If all recent runs also had 0 new filings, log WARNING: "Zero new filings for {N} consecutive runs -- REGDOCS site structure may have changed"
       - (RunHistory record creation itself happens in Phase 9 pipeline orchestration -- this function just checks and warns)

    10. **Return ScrapeResult** with all counts.

    **Module exports in __init__.py:**
    ```python
    __all__ = ["scrape_recent_filings", "ScrapeResult"]
    ```

    Also import and re-export from submodules for convenience:
    ```python
    from .models import ScrapedFiling, ScrapedDocument
    ```

    **Error handling:**
    - Wrap the entire orchestration in try/except at the top level
    - Individual filing persistence failures should not crash the batch -- log and continue
    - If both API and DOM strategies fail completely, return ScrapeResult with errors list populated
    - Never let the orchestrator raise an exception that would crash the pipeline

    **Logging:**
    - INFO: scrape start, strategy used, total found, new persisted, scrape complete
    - DEBUG: each filing processed (id, strategy, filtered/skipped/persisted)
    - WARNING: zero filings found, consecutive zero runs, validation failures, both strategies failed
    - ERROR: robots.txt blocked, database errors
  </action>
  <verify>
    - `uv run python -c "from cer_scraper.scraper import scrape_recent_filings, ScrapeResult; print('Import OK')"` succeeds
    - `uv run python -c "from cer_scraper.scraper import ScrapedFiling, ScrapedDocument; print('Models re-exported OK')"` succeeds
    - Code inspection: robots.txt checked before any scraping
    - Code inspection: API discovery attempted first, DOM parsing as fallback
    - Code inspection: filing_exists() called for deduplication
    - Code inspection: Filings with no documents skipped
    - Code inspection: Config filters applied (filing_type_include/exclude, applicant_filter, proceeding_filter)
    - Code inspection: Zero-filing consecutive run warning implemented
    - Code inspection: Top-level try/except prevents crashes
    - Code inspection: create_filing() called with placeholder values for missing metadata
    - `uv run python main.py` -- existing pipeline still starts and completes (no regressions from new imports)
  </verify>
  <done>Scraper orchestrator implements the full flow: robots.txt check -> API discovery -> API client (or DOM fallback) -> validation -> filtering -> dedup -> persistence -> zero-filing tracking. Returns ScrapeResult with detailed counts. Never crashes -- all errors logged and returned in result.</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from cer_scraper.scraper import scrape_recent_filings, ScrapeResult, ScrapedFiling, ScrapedDocument; print('All public API imports OK')"` -- public API accessible
2. `uv run python -c "from cer_scraper.scraper.dom_parser import parse_filings_from_html; print('DOM parser OK')"` -- fallback importable
3. Code review: orchestrator flow matches the 10-step sequence above
4. Code review: both strategies produce ScrapedFiling models (same output type)
5. Code review: all user decisions from CONTEXT.md implemented:
   - API-first with DOM fallback
   - Discovery retries before fallback
   - Configurable lookback period, filing type filters, applicant/proceeding filters
   - Deduplication via state store
   - Randomized 1-3s delays
   - 3 retries with exponential backoff
   - Zero-filing warning after 3 consecutive runs
   - Validation checks after scrape
   - Skip filings with no document URLs
   - Capture all document types (not just PDFs)
   - Placeholders for missing metadata
6. `uv run python main.py` -- no regressions
</verification>

<success_criteria>
- DOM parser extracts filing metadata from rendered HTML with multiple parsing strategies
- Scraper orchestrator provides clean `scrape_recent_filings()` public API
- API-first strategy with DOM fallback works end-to-end
- Filtering by filing type, applicant, proceeding number from config
- Deduplication against state store prevents duplicate filings
- Filings without document URLs are skipped
- Filings with partial metadata stored with placeholders
- Zero-filing consecutive run warning fires after 3 runs
- Rate limiting applied between all requests
- robots.txt respected
- All errors handled gracefully -- orchestrator never crashes
- Existing Phase 1 code (main.py, config, DB) still works
</success_criteria>

<output>
After completion, create `.planning/phases/02-regdocs-scraper/02-03-SUMMARY.md`
</output>
