---
phase: 01-foundation-configuration
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - main.py
  - src/cer_scraper/db/state.py
autonomous: true

must_haves:
  truths:
    - "Running main.py creates a SQLite database at data/state.db with all tables and produces log output to both console and logs/pipeline.log"
    - "Marking a filing as processed in the state store prevents it from appearing in unprocessed queries"
    - "A filing with retry_count >= max_retry_count is excluded from unprocessed queries"
    - "The database and log files persist across application restarts"
  artifacts:
    - path: "main.py"
      provides: "Application entry point that wires logging, config, and database together"
      contains: "setup_logging"
    - path: "src/cer_scraper/db/state.py"
      provides: "State store operations: query unprocessed filings, mark steps complete"
      contains: "get_unprocessed_filings"
  key_links:
    - from: "main.py"
      to: "src/cer_scraper/logging/setup.py"
      via: "setup_logging() called first"
      pattern: "setup_logging"
    - from: "main.py"
      to: "src/cer_scraper/config/settings.py"
      via: "PipelineSettings loaded for db_path and log settings"
      pattern: "PipelineSettings"
    - from: "main.py"
      to: "src/cer_scraper/db/engine.py"
      via: "get_engine + init_db + get_session_factory"
      pattern: "init_db"
    - from: "src/cer_scraper/db/state.py"
      to: "src/cer_scraper/db/models.py"
      via: "queries Filing model for unprocessed filings"
      pattern: "select\\(Filing\\)"
---

<objective>
Wire all foundation components together into a working entry point and implement state store operations, completing the Phase 1 foundation that all downstream phases build on.

Purpose: This plan proves the foundation works end-to-end: config loads, database initializes, logging outputs correctly, and state queries function. Without this integration, the individual components are untested in combination.
Output: A working main.py entry point and state store module with query/update operations.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-configuration/01-RESEARCH.md
@.planning/phases/01-foundation-configuration/01-02-SUMMARY.md
@.planning/phases/01-foundation-configuration/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create state store operations module</name>
  <files>
    src/cer_scraper/db/state.py
  </files>
  <action>
Create `src/cer_scraper/db/state.py` with functions for querying and updating filing state. These are the core state management operations that every pipeline stage will use.

**Functions to implement:**

`get_unprocessed_filings(session: Session, max_retries: int = 3) -> list[Filing]`:
- Returns filings where `status_emailed != "success"` AND `retry_count < max_retries`
- This means: filings that have not completed the full pipeline AND have not exceeded the retry limit
- Uses `select(Filing).where(...)` with `session.scalars().all()`
- Returns a list (not a query/result object)

`get_filing_by_id(session: Session, filing_id: str) -> Filing | None`:
- Looks up a filing by its REGDOCS filing_id (not the database primary key)
- Returns None if not found
- Uses `select(Filing).where(Filing.filing_id == filing_id)` with `session.scalars().first()`

`mark_step_complete(session: Session, filing_id: str, step: str, status: str = "success", error: str | None = None) -> None`:
- Updates the status column for a specific pipeline step
- `step` must be one of: "scraped", "downloaded", "extracted", "analyzed", "emailed"
- Sets `status_{step}` to the given status value
- If error is provided, sets `error_message` and increments `retry_count`
- Calls `session.commit()` to persist the change
- Raises ValueError if step is not in the valid list
- Logs at DEBUG level what was updated

`create_filing(session: Session, filing_id: str, **kwargs) -> Filing`:
- Creates a new Filing record with the given filing_id and any additional fields from kwargs
- Sets `status_scraped = "success"` (since we just scraped it)
- Calls `session.add(filing)` and `session.commit()`
- Returns the created Filing object
- Logs at INFO level: "Created filing {filing_id}"

`filing_exists(session: Session, filing_id: str) -> bool`:
- Returns True if a filing with the given filing_id exists in the database
- Efficient check using `select(Filing.id).where(Filing.filing_id == filing_id)` with `.first()`

**Imports:**
```python
import logging
from sqlalchemy import select
from sqlalchemy.orm import Session
from .models import Filing

logger = logging.getLogger(__name__)

VALID_STEPS = ("scraped", "downloaded", "extracted", "analyzed", "emailed")
```

**Per RESEARCH.md Pitfall 3:** Always call `session.commit()` explicitly. Without it, changes are lost when the session closes.

Also update `src/cer_scraper/db/__init__.py` to export state functions:
```python
from .state import get_unprocessed_filings, get_filing_by_id, mark_step_complete, create_filing, filing_exists
```
  </action>
  <verify>
Run: `uv run python -c "
import tempfile, os
from cer_scraper.db import get_engine, init_db, get_session_factory
from cer_scraper.db import create_filing, get_unprocessed_filings, mark_step_complete, filing_exists, get_filing_by_id

# Setup temp DB
tmp = tempfile.mkdtemp()
db_path = os.path.join(tmp, 'test.db')
engine = get_engine(db_path)
init_db(engine)
Session = get_session_factory(engine)

with Session() as session:
    # Test 1: Create a filing
    f = create_filing(session, 'REG-001', applicant='Test Corp', filing_type='Application')
    assert f.filing_id == 'REG-001'
    assert f.status_scraped == 'success'
    assert f.status_emailed == 'pending'

    # Test 2: filing_exists
    assert filing_exists(session, 'REG-001') == True
    assert filing_exists(session, 'REG-999') == False

    # Test 3: get_unprocessed_filings returns our filing
    unprocessed = get_unprocessed_filings(session)
    assert len(unprocessed) == 1
    assert unprocessed[0].filing_id == 'REG-001'

    # Test 4: Mark all steps complete
    for step in ['downloaded', 'extracted', 'analyzed', 'emailed']:
        mark_step_complete(session, 'REG-001', step, 'success')

    # Test 5: Fully processed filing no longer appears as unprocessed
    unprocessed = get_unprocessed_filings(session)
    assert len(unprocessed) == 0, f'Expected 0 unprocessed, got {len(unprocessed)}'

    # Test 6: Create a second filing and mark it as failed
    create_filing(session, 'REG-002', applicant='Fail Corp')
    for i in range(3):
        mark_step_complete(session, 'REG-002', 'downloaded', 'failed', error=f'Error {i}')

    # Filing with 3 retries should be excluded
    unprocessed = get_unprocessed_filings(session, max_retries=3)
    assert len(unprocessed) == 0, f'Expected 0 (retry limit), got {len(unprocessed)}'

print('All state store tests passed')

# Cleanup
engine.dispose()
import shutil
shutil.rmtree(tmp)
"
`
  </verify>
  <done>State store module provides get_unprocessed_filings, mark_step_complete, create_filing, filing_exists, and get_filing_by_id. Marking all steps complete removes a filing from unprocessed queries. Retry limit exclusion works.</done>
</task>

<task type="auto">
  <name>Task 2: Wire main.py entry point</name>
  <files>
    main.py
  </files>
  <action>
Replace the hello-world `main.py` with the application entry point that wires together logging, configuration, and database initialization. Follow the startup order from RESEARCH.md (Pitfall 4: logging must be configured FIRST).

**Startup sequence:**

1. **Setup logging FIRST** -- import and call `setup_logging()` before anything else that might log. Use PipelineSettings defaults initially (log_dir="logs"), then optionally reconfigure after config loads if needed.

2. **Load configuration** -- import PipelineSettings, ScraperSettings, EmailSettings. Log the loaded config values (excluding secrets).

3. **Initialize database** -- call `get_engine(pipeline.db_path)`, then `init_db(engine)`, then `get_session_factory(engine)`.

4. **Log readiness** -- log that the application is ready, report the database path and number of unprocessed filings.

5. **Placeholder for pipeline** -- add a comment indicating where pipeline stages (scrape, download, extract, analyze, email) will be wired in Phase 9.

**Implementation:**

```python
# main.py
import logging
from cer_scraper.logging import setup_logging
from cer_scraper.config.settings import PipelineSettings, ScraperSettings, EmailSettings
from cer_scraper.db import get_engine, init_db, get_session_factory, get_unprocessed_filings

def main():
    # 1. Load pipeline config first (needed for log_dir)
    pipeline = PipelineSettings()

    # 2. Setup logging FIRST before any log output
    setup_logging(
        log_dir=pipeline.log_dir,
        max_bytes=pipeline.log_max_bytes,
        backup_count=pipeline.log_backup_count,
    )

    logger = logging.getLogger(__name__)
    logger.info("CER REGDOCS Scraper starting")

    # 3. Load remaining config
    scraper = ScraperSettings()
    email = EmailSettings()
    logger.info(
        "Configuration loaded",
        extra={
            "db_path": pipeline.db_path,
            "scraper_url": scraper.base_url,
            "email_smtp": email.smtp_host,
        }
    )

    # 4. Initialize database
    engine = get_engine(pipeline.db_path)
    init_db(engine)
    SessionFactory = get_session_factory(engine)
    logger.info("Database initialized at %s", pipeline.db_path)

    # 5. Report status
    with SessionFactory() as session:
        unprocessed = get_unprocessed_filings(session, max_retries=pipeline.max_retry_count)
        logger.info(
            "Application ready. %d unprocessed filings pending.",
            len(unprocessed),
        )

    # Pipeline stages will be wired here in Phase 9
    # - Phase 2: Scrape REGDOCS for new filings
    # - Phase 3: Download PDFs
    # - Phase 4: Extract text from PDFs
    # - Phase 5-6: Analyze filings with Claude CLI
    # - Phase 8: Send email reports

    logger.info("Run complete")


if __name__ == "__main__":
    main()
```

**Important details:**
- PipelineSettings is loaded before setup_logging because we need `log_dir`, `log_max_bytes`, and `log_backup_count` from config to pass to the logging setup
- Do NOT log secrets (email app_password). Only log non-sensitive config values.
- The `extra={}` dict in log calls adds structured fields to the JSON log output
- Use `%s` string formatting in logger calls (not f-strings) -- this is the logging best practice for lazy evaluation

After writing main.py, run it end-to-end:
```bash
uv run python main.py
```

Verify:
1. Console shows human-readable log output with timestamps and component names
2. `data/state.db` file is created
3. `logs/pipeline.log` file is created with JSON log entries
4. No errors in output
5. Running it a second time still works (idempotent -- does not crash on existing DB/tables)
  </action>
  <verify>
Run these checks in sequence:
1. `uv run python main.py` -- should exit cleanly with log output showing "Application ready. 0 unprocessed filings pending." and "Run complete"
2. Verify data/state.db exists: `uv run python -c "from pathlib import Path; assert Path('data/state.db').exists(); print('DB exists')"`
3. Verify logs/pipeline.log exists with JSON: `uv run python -c "from pathlib import Path; import json; lines = Path('logs/pipeline.log').read_text().strip().split('\n'); [json.loads(l) for l in lines]; print(f'{len(lines)} valid JSON log entries')"`
4. Run main.py a second time to confirm idempotency: `uv run python main.py` -- no errors
  </verify>
  <done>main.py wires logging (first), config, and database together. Running it creates data/state.db with all tables, logs/pipeline.log with JSON entries, and produces readable console output. The application starts, reports status, and exits cleanly. Idempotent across restarts.</done>
</task>

</tasks>

<verification>
Run the complete Phase 1 success criteria checks:

1. **SC-1 (Database persists):** `uv run python main.py` creates `data/state.db`. Run it again -- DB still has tables, no errors.

2. **SC-2 (Logging):** Console output has timestamps, levels, component names. `logs/pipeline.log` contains JSON with same fields. File rotation is configured (check via code inspection -- maxBytes=10MB, backupCount=5).

3. **SC-3 (Config externalized):** Secrets are in .env.example (not in YAML). Settings come from config/*.yaml. Neither is hardcoded in Python source.

4. **SC-4 (Data model):** Filing model has filing_id, date, applicant, filing_type, proceeding_number. Documents table links to filings with PDF URLs.

5. **SC-5 (State tracking):** `uv run python -c "
import tempfile, os
from cer_scraper.db import get_engine, init_db, get_session_factory, create_filing, get_unprocessed_filings, mark_step_complete
tmp = tempfile.mkdtemp(); db = os.path.join(tmp, 't.db')
e = get_engine(db); init_db(e); S = get_session_factory(e)
with S() as s:
    create_filing(s, 'TEST-1')
    assert len(get_unprocessed_filings(s)) == 1
    for step in ['downloaded','extracted','analyzed','emailed']:
        mark_step_complete(s, 'TEST-1', step)
    assert len(get_unprocessed_filings(s)) == 0
    print('State tracking: PASS')
e.dispose(); import shutil; shutil.rmtree(tmp)
"`
</verification>

<success_criteria>
- main.py runs without errors and produces both console and file log output
- data/state.db is created automatically on first run with all 4 tables
- logs/pipeline.log contains valid JSON log entries with timestamp, level, component, message
- State store correctly filters out fully-processed filings and retry-exhausted filings
- Application is idempotent -- second run works identically to first
- All Phase 1 roadmap success criteria are satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-configuration/01-04-SUMMARY.md`
</output>
