---
phase: 01-foundation-configuration
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/cer_scraper/db/models.py
  - src/cer_scraper/db/engine.py
  - src/cer_scraper/db/__init__.py
  - src/cer_scraper/logging/__init__.py
  - src/cer_scraper/logging/setup.py
autonomous: true

must_haves:
  truths:
    - "Running init_db creates a SQLite database file at the configured path with all four tables (filings, documents, analyses, run_history)"
    - "The Filing model has per-step status columns for scraped, downloaded, extracted, analyzed, and emailed"
    - "Filing model includes error_message and retry_count fields for failure tracking"
    - "Logging produces JSON output to a rotating file and human-readable output to console"
    - "Log rotation creates new files when the 10MB size limit is reached"
  artifacts:
    - path: "src/cer_scraper/db/models.py"
      provides: "SQLAlchemy ORM models: Filing, Document, Analysis, RunHistory, Base"
      contains: "class Filing"
    - path: "src/cer_scraper/db/engine.py"
      provides: "Engine factory, session factory, and init_db function"
      contains: "def init_db"
    - path: "src/cer_scraper/logging/setup.py"
      provides: "Dual-handler logging setup (JSON file + text console)"
      contains: "RotatingFileHandler"
  key_links:
    - from: "src/cer_scraper/db/engine.py"
      to: "src/cer_scraper/db/models.py"
      via: "imports Base for create_all()"
      pattern: "from .models import Base"
    - from: "src/cer_scraper/db/engine.py"
      to: "data/state.db"
      via: "SQLite engine URI"
      pattern: "sqlite:///"
    - from: "src/cer_scraper/logging/setup.py"
      to: "logs/pipeline.log"
      via: "RotatingFileHandler filename"
      pattern: "pipeline\\.log"
---

<objective>
Build the database layer (ORM models, engine, session factory) and logging infrastructure so the application can persist filing state and produce structured log output.

Purpose: The database models define the core data structures that every downstream phase operates on. Logging is critical for debugging a batch pipeline that runs unattended. Both are foundational and have no dependencies on each other, so they ship together as a Wave 2 plan parallel to configuration.
Output: Four SQLAlchemy models (Filing, Document, Analysis, RunHistory), engine/session factory, and dual-handler logging setup.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-configuration/01-RESEARCH.md
@.planning/phases/01-foundation-configuration/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SQLAlchemy ORM models</name>
  <files>
    src/cer_scraper/db/models.py
  </files>
  <action>
Create `src/cer_scraper/db/models.py` with four SQLAlchemy 2.0 ORM models using `DeclarativeBase`, `Mapped[]`, and `mapped_column()`. Follow the exact schema from RESEARCH.md Pattern 2.

**Base class:**
```python
class Base(DeclarativeBase):
    pass
```

**Filing model (`filings` table):**
- `id: Mapped[int]` -- primary key (auto-increment)
- `filing_id: Mapped[str]` -- unique, indexed, String(100) -- the REGDOCS filing identifier
- `date: Mapped[Optional[datetime.date]]` -- filing date
- `applicant: Mapped[Optional[str]]` -- String(500)
- `filing_type: Mapped[Optional[str]]` -- String(200) -- application, decision, order, etc.
- `proceeding_number: Mapped[Optional[str]]` -- String(100), indexed
- `title: Mapped[Optional[str]]` -- String(1000)
- `url: Mapped[Optional[str]]` -- String(2000) -- REGDOCS URL
- Per-step status columns (all String(20), default "pending"):
  - `status_scraped`
  - `status_downloaded`
  - `status_extracted`
  - `status_analyzed`
  - `status_emailed`
- `error_message: Mapped[Optional[str]]` -- Text, stores last error
- `retry_count: Mapped[int]` -- default 0
- `created_at: Mapped[datetime.datetime]` -- server_default=func.now()
- `updated_at: Mapped[Optional[datetime.datetime]]` -- onupdate=func.now()
- Relationships: `documents` (list of Document, cascade all/delete-orphan), `analyses` (list of Analysis, cascade all/delete-orphan)
- `__repr__` returning filing_id and applicant

**Document model (`documents` table):**
- `id: Mapped[int]` -- primary key
- `filing_id: Mapped[int]` -- ForeignKey("filings.id")
- `document_url: Mapped[str]` -- String(2000)
- `filename: Mapped[Optional[str]]` -- String(500)
- `local_path: Mapped[Optional[str]]` -- String(1000)
- `download_status: Mapped[str]` -- String(20), default "pending"
- `file_size_bytes: Mapped[Optional[int]]`
- `content_type: Mapped[Optional[str]]` -- String(100)
- `created_at: Mapped[datetime.datetime]` -- server_default=func.now()
- Relationship: `filing` back to Filing
- `__repr__` returning filename and status

**Analysis model (`analyses` table):**
- `id: Mapped[int]` -- primary key
- `filing_id: Mapped[int]` -- ForeignKey("filings.id")
- `analysis_type: Mapped[str]` -- String(50) -- "full", "summary", "entity"
- `output: Mapped[Optional[str]]` -- Text, stores analysis JSON
- `status: Mapped[str]` -- String(20), default "pending"
- `error_message: Mapped[Optional[str]]` -- Text
- `duration_seconds: Mapped[Optional[float]]`
- `created_at: Mapped[datetime.datetime]` -- server_default=func.now()
- Relationship: `filing` back to Filing
- `__repr__` returning type and status

**RunHistory model (`run_history` table):**
- `id: Mapped[int]` -- primary key
- `started_at: Mapped[datetime.datetime]` -- server_default=func.now()
- `completed_at: Mapped[Optional[datetime.datetime]]`
- `total_filings_found: Mapped[int]` -- default 0
- `new_filings: Mapped[int]` -- default 0
- `processed_ok: Mapped[int]` -- default 0
- `processed_failed: Mapped[int]` -- default 0
- `duration_seconds: Mapped[Optional[float]]`
- `__repr__` returning id and started_at

Use these imports:
```python
import datetime
from typing import Optional
from sqlalchemy import ForeignKey, String, Text, func
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship
```

Do NOT use the legacy `Column()` or `declarative_base()` patterns. Use modern SQLAlchemy 2.0 syntax exclusively per RESEARCH.md guidance.
  </action>
  <verify>
Run: `uv run python -c "
from cer_scraper.db.models import Base, Filing, Document, Analysis, RunHistory
# Verify all tables registered
tables = Base.metadata.tables
assert 'filings' in tables, 'filings table missing'
assert 'documents' in tables, 'documents table missing'
assert 'analyses' in tables, 'analyses table missing'
assert 'run_history' in tables, 'run_history table missing'
# Verify Filing has per-step status columns
filing_cols = [c.name for c in tables['filings'].columns]
for step in ['status_scraped', 'status_downloaded', 'status_extracted', 'status_analyzed', 'status_emailed']:
    assert step in filing_cols, f'{step} column missing from filings'
assert 'error_message' in filing_cols
assert 'retry_count' in filing_cols
print('All 4 models defined with correct columns')
"
`
  </verify>
  <done>Four SQLAlchemy 2.0 ORM models exist with all columns matching the locked schema decisions. Filing has per-step status tracking, error recording, and retry count. Documents and analyses link to filings via foreign keys.</done>
</task>

<task type="auto">
  <name>Task 2: Create database engine, session factory, and init_db</name>
  <files>
    src/cer_scraper/db/engine.py
    src/cer_scraper/db/__init__.py
  </files>
  <action>
Create `src/cer_scraper/db/engine.py` following RESEARCH.md Pattern 3.

**Functions:**

`get_engine(db_path: str = "data/state.db") -> Engine`:
- Ensure parent directory exists: `Path(db_path).parent.mkdir(parents=True, exist_ok=True)`
- Create and return engine: `create_engine(f"sqlite:///{db_path}", echo=False)`
- Use absolute path resolution to avoid Pitfall 5 (SQLite file path not created)

`init_db(engine) -> None`:
- Call `Base.metadata.create_all(engine)` -- idempotent, only creates tables that do not exist
- Log at INFO level: "Database tables initialized"
- Use `logging.getLogger(__name__)` for the logger

`get_session_factory(engine) -> sessionmaker[Session]`:
- Return `sessionmaker(bind=engine)`

**Imports:**
```python
import logging
from pathlib import Path
from sqlalchemy import create_engine, Engine
from sqlalchemy.orm import sessionmaker, Session
from .models import Base
```

Update `src/cer_scraper/db/__init__.py` to export key items:
```python
from .models import Base, Filing, Document, Analysis, RunHistory
from .engine import get_engine, init_db, get_session_factory
```

**Important per RESEARCH.md anti-patterns:**
- Create the engine ONCE, not per-request
- Always ensure parent directory exists before creating SQLite engine
- The `echo=False` default avoids noisy SQL output; set to True only for debugging
  </action>
  <verify>
Run: `uv run python -c "
import tempfile, os
from pathlib import Path
from cer_scraper.db import get_engine, init_db, get_session_factory, Filing

# Create DB in temp dir
tmp = tempfile.mkdtemp()
db_path = os.path.join(tmp, 'test.db')
engine = get_engine(db_path)
init_db(engine)

# Verify file exists
assert Path(db_path).exists(), 'Database file not created'

# Verify tables exist
from sqlalchemy import inspect
inspector = inspect(engine)
table_names = inspector.get_table_names()
assert 'filings' in table_names, 'filings table not created'
assert 'documents' in table_names, 'documents table not created'
assert 'analyses' in table_names, 'analyses table not created'
assert 'run_history' in table_names, 'run_history table not created'

# Verify session factory works
Session = get_session_factory(engine)
with Session() as session:
    filings = session.query(Filing).all()
    assert filings == [], 'Expected empty filings list'

print(f'Database created at {db_path} with all 4 tables. Session works.')

# Cleanup
engine.dispose()
os.remove(db_path)
os.rmdir(tmp)
"
`
  </verify>
  <done>get_engine creates SQLite engine with parent directory creation. init_db creates all 4 tables idempotently. get_session_factory returns a working session maker. All functions exported from db package.</done>
</task>

<task type="auto">
  <name>Task 3: Create dual-handler logging setup</name>
  <files>
    src/cer_scraper/logging/setup.py
    src/cer_scraper/logging/__init__.py
  </files>
  <action>
Create `src/cer_scraper/logging/setup.py` following RESEARCH.md Pattern 4.

**Function: `setup_logging(...) -> None`**

Parameters (all with defaults from discretion decisions):
- `log_dir: str = "logs"` -- directory for log files
- `log_level_file: int = logging.DEBUG` -- file handler captures everything
- `log_level_console: int = logging.INFO` -- console shows progress/warnings/errors
- `max_bytes: int = 10_485_760` -- 10MB per file (discretion decision)
- `backup_count: int = 5` -- 5 rotated backups (discretion decision)

Implementation:
1. Create log directory: `Path(log_dir).mkdir(parents=True, exist_ok=True)`
2. Get root logger, set to DEBUG (capture everything; handlers filter)
3. **Clear any existing handlers** to prevent duplicate output if called multiple times
4. Create `RotatingFileHandler`:
   - filename: `Path(log_dir) / "pipeline.log"`
   - maxBytes: `max_bytes`
   - backupCount: `backup_count`
   - encoding: "utf-8"
   - Level: `log_level_file`
   - Formatter: `JsonFormatter` from pythonjsonlogger with fields: timestamp, component (from logger name), level, message
5. Create `StreamHandler` (console):
   - Level: `log_level_console`
   - Formatter: text format `"%(asctime)s [%(levelname)-8s] %(name)s: %(message)s"` with datefmt `"%Y-%m-%d %H:%M:%S"`
6. Add both handlers to root logger

**JSON formatter configuration per RESEARCH.md:**
```python
from pythonjsonlogger.json import JsonFormatter

json_formatter = JsonFormatter(
    fmt="%(asctime)s %(name)s %(levelname)s %(message)s",
    rename_fields={"asctime": "timestamp", "levelname": "level", "name": "component"},
    datefmt="%Y-%m-%dT%H:%M:%S",
)
```

Update `src/cer_scraper/logging/__init__.py`:
```python
from .setup import setup_logging
```

**Per RESEARCH.md anti-patterns:**
- Do NOT use `logging.basicConfig()` -- we need explicit handler configuration
- This function should be called FIRST in main() before any other imports that might log (Pitfall 4)
- Module code throughout the project should use `logging.getLogger(__name__)` and never configure handlers
  </action>
  <verify>
Run: `uv run python -c "
import tempfile, os, json, logging
from pathlib import Path
from cer_scraper.logging import setup_logging

# Setup with temp log dir
tmp_log_dir = tempfile.mkdtemp()
setup_logging(log_dir=tmp_log_dir)

# Create a test logger (simulating module usage)
logger = logging.getLogger('cer_scraper.test')
logger.info('Test info message')
logger.debug('Test debug message')

# Verify log file exists
log_file = Path(tmp_log_dir) / 'pipeline.log'
assert log_file.exists(), 'Log file not created'

# Verify JSON format in file
lines = log_file.read_text(encoding='utf-8').strip().split('\n')
assert len(lines) >= 2, f'Expected 2+ log lines, got {len(lines)}'
for line in lines:
    data = json.loads(line)
    assert 'timestamp' in data, 'Missing timestamp field'
    assert 'level' in data, 'Missing level field'
    assert 'component' in data, 'Missing component field'
    assert 'message' in data, 'Missing message field'

print(f'Logging setup OK. {len(lines)} JSON log entries in {log_file}')
print(f'Sample: {lines[0][:100]}...')

# Cleanup
import shutil
logging.getLogger().handlers.clear()
shutil.rmtree(tmp_log_dir)
"
`
  </verify>
  <done>setup_logging() creates dual handlers: JSON rotating file (DEBUG level, 10MB, 5 backups) and text console (INFO level). Log file at logs/pipeline.log contains valid JSON with timestamp, component, level, and message fields. Exported from logging package.</done>
</task>

</tasks>

<verification>
1. All 4 SQLAlchemy models import correctly: `from cer_scraper.db import Filing, Document, Analysis, RunHistory`
2. Database creation in temp directory produces 4 tables with correct columns
3. Session factory creates working sessions that can query empty tables
4. Logging produces JSON file output and text console output
5. Log rotation is configured at 10MB with 5 backups
6. Filing model has all 5 per-step status columns, error_message, and retry_count
</verification>

<success_criteria>
- Four ORM models match the locked schema decisions (per-step status, failure tracking, run history)
- init_db creates all tables idempotently on a fresh SQLite database
- Logging setup produces JSON in files and human-readable text on console
- All modules use logging.getLogger(__name__) pattern (no basicConfig)
- Database and log directories are created automatically if missing
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-configuration/01-03-SUMMARY.md`
</output>
