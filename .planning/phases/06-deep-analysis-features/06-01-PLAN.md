---
phase: 06-deep-analysis-features
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cer_scraper/analyzer/schemas.py
  - src/cer_scraper/analyzer/prompt.py
autonomous: true

must_haves:
  truths:
    - "AnalysisOutput schema accepts Phase 5 output (backward compat) AND Phase 6 output (all new fields)"
    - "Phase 6 fields all have defaults so existing Phase 5 analysis_json validates without error"
    - "regulatory_implications is Optional (None for routine filings)"
    - "dates is always a list (empty array if no dates)"
    - "sentiment, impact are Optional with None defaults for backward compat"
    - "get_json_schema_description includes all Phase 6 fields in the human-readable example"
    - "build_prompt accepts analysis_date as a new required parameter"
  artifacts:
    - path: "src/cer_scraper/analyzer/schemas.py"
      provides: "RegulatoryImplications, ExtractedDate, SentimentAssessment, RepresentativeQuote, ImpactScore models + extended AnalysisOutput"
      contains: "class RegulatoryImplications"
    - path: "src/cer_scraper/analyzer/prompt.py"
      provides: "Updated get_json_schema_description with Phase 6 fields, build_prompt with analysis_date param"
      contains: "analysis_date"
  key_links:
    - from: "src/cer_scraper/analyzer/prompt.py"
      to: "src/cer_scraper/analyzer/schemas.py"
      via: "get_json_schema_description output must match AnalysisOutput field names exactly"
      pattern: "regulatory_implications.*dates.*sentiment.*quotes.*impact"
---

<objective>
Extend the Pydantic schema and prompt infrastructure to support five new analysis dimensions: regulatory implications, date extraction, sentiment assessment, representative quotes, and impact scoring.

Purpose: Establish the data contract (schemas.py) and JSON schema description (prompt.py) that the enriched prompt template and service wiring (Plan 02) will depend on. All Phase 6 fields use defaults for backward compatibility with existing Phase 5 data.

Output: Extended AnalysisOutput schema with 5 new sub-models, updated get_json_schema_description(), and updated build_prompt() signature with analysis_date parameter.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-deep-analysis-features/06-CONTEXT.md
@.planning/phases/06-deep-analysis-features/06-RESEARCH.md
@src/cer_scraper/analyzer/schemas.py
@src/cer_scraper/analyzer/prompt.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Phase 6 Pydantic models and extend AnalysisOutput</name>
  <files>src/cer_scraper/analyzer/schemas.py</files>
  <action>
Add five new Pydantic models to schemas.py AFTER the existing Classification model and BEFORE AnalysisOutput:

1. **RegulatoryImplications(BaseModel):**
   - `summary: str` -- 2-3 sentences describing regulatory impact
   - `affected_parties: list[str] = Field(default_factory=list)` -- specific names or general categories

2. **ExtractedDate(BaseModel):**
   - `date: str` -- ISO 8601 preferred, but accepts descriptive text like "Q1 2026" or "within 30 days" (use str, NOT datetime.date)
   - `type: str` -- one of "deadline", "hearing", "comment_period", "effective", "filing", "other"
   - `description: str` -- what this date refers to
   - `temporal_status: str` -- one of "past", "upcoming", "today"

3. **SentimentAssessment(BaseModel):**
   - `category: str` -- one of "routine", "notable", "urgent", "adversarial", "cooperative"
   - `nuance: str` -- free-form description (e.g. "cautiously supportive")

4. **RepresentativeQuote(BaseModel):**
   - `text: str` -- the quote text (1-2 sentences)
   - `source_location: str | None = None` -- page/section if identifiable, None otherwise

5. **ImpactScore(BaseModel):**
   - `score: int = Field(ge=1, le=5)` -- 1=informational, 5=immediate attention
   - `justification: str` -- 1-2 sentence explanation

Then extend the existing AnalysisOutput model by adding these fields AFTER `key_facts`:

```python
# Phase 6 fields (all have defaults for backward compatibility with Phase 5 data)
regulatory_implications: RegulatoryImplications | None = None
dates: list[ExtractedDate] = Field(default_factory=list)
sentiment: SentimentAssessment | None = None
quotes: list[RepresentativeQuote] = Field(default_factory=list)
impact: ImpactScore | None = None
```

Per user decision: regulatory_implications is the only conditionally-null field (null for routine filings). sentiment and impact use None defaults solely for backward compat -- new analyses will always populate them.

Update the AnalysisOutput docstring to document the Phase 6 fields.

Add each new model with a brief docstring explaining its purpose.
  </action>
  <verify>
Run: `cd C:\Users\amelv\Repo\cer_repository_scrapper && uv run python -c "
from cer_scraper.analyzer.schemas import (
    AnalysisOutput, RegulatoryImplications, ExtractedDate,
    SentimentAssessment, RepresentativeQuote, ImpactScore
)
# Test Phase 5 backward compat
p5 = AnalysisOutput.model_validate({
    'summary': 'test', 'entities': [], 'relationships': [],
    'classification': {'primary_type': 'Order', 'tags': [], 'confidence': 80, 'justification': 'test'},
    'key_facts': ['fact']
})
assert p5.regulatory_implications is None
assert p5.dates == []
assert p5.sentiment is None
assert p5.impact is None
# Test Phase 6 full output
p6 = AnalysisOutput.model_validate({
    'summary': 'test', 'entities': [], 'relationships': [],
    'classification': {'primary_type': 'Application', 'tags': [], 'confidence': 90, 'justification': 'test'},
    'key_facts': ['fact'],
    'regulatory_implications': {'summary': 'Impact desc', 'affected_parties': ['TC Energy']},
    'dates': [{'date': '2026-03-15', 'type': 'deadline', 'description': 'Comment closes', 'temporal_status': 'upcoming'}],
    'sentiment': {'category': 'routine', 'nuance': 'standard admin'},
    'quotes': [{'text': 'The Board orders...', 'source_location': None}],
    'impact': {'score': 3, 'justification': 'Moderate significance.'}
})
assert p6.impact.score == 3
assert p6.regulatory_implications.affected_parties == ['TC Energy']
assert p6.dates[0].temporal_status == 'upcoming'
# Test null implications for routine
p6_routine = AnalysisOutput.model_validate({
    'summary': 'test', 'entities': [], 'relationships': [],
    'classification': {'primary_type': 'Correspondence', 'tags': [], 'confidence': 80, 'justification': 'test'},
    'key_facts': [],
    'regulatory_implications': None,
    'dates': [],
    'sentiment': {'category': 'routine', 'nuance': 'standard'},
    'quotes': [],
    'impact': {'score': 1, 'justification': 'Informational.'}
})
assert p6_routine.regulatory_implications is None
print('All schema validations passed')
"`
  </verify>
  <done>Five new Pydantic models exist in schemas.py. AnalysisOutput validates both Phase 5 output (missing Phase 6 fields) and full Phase 6 output. ImpactScore.score constrained to 1-5. regulatory_implications accepts None for routine filings.</done>
</task>

<task type="auto">
  <name>Task 2: Update prompt.py JSON schema description and build_prompt signature</name>
  <files>src/cer_scraper/analyzer/prompt.py</files>
  <action>
Two changes to prompt.py:

**1. Extend get_json_schema_description():**

Add the Phase 6 fields to the returned string, AFTER the existing `key_facts` section. The description must use the EXACT field names matching the Pydantic models (regulatory_implications, dates, sentiment, quotes, impact). Format as a readable JSON example with inline comments:

```
  "regulatory_implications": {
    "summary": "2-3 sentences describing the regulatory impact. What does this filing mean?",
    "affected_parties": ["Specific company/community names when mentioned, or general categories like 'pipeline operators'"]
  },

  "dates": [
    {
      "date": "2026-03-15",
      "type": "deadline | hearing | comment_period | effective | filing | other",
      "description": "What this date refers to",
      "temporal_status": "past | upcoming | today"
    }
  ],

  "sentiment": {
    "category": "routine | notable | urgent | adversarial | cooperative",
    "nuance": "Free-form description of tone, e.g. 'cautiously supportive with procedural concerns'"
  },

  "quotes": [
    {
      "text": "1-2 sentence quote that captures a key point from the filing",
      "source_location": "Document name, page number, or section heading (or null)"
    }
  ],

  "impact": {
    "score": 3,
    "justification": "1-2 sentence explanation of why this score was assigned"
  }
```

Ensure the closing `}` of the overall JSON object is correct. The returned string must be valid-looking JSON (it is a human-readable example, not parsed, but it should look correct).

**2. Add analysis_date parameter to build_prompt():**

Add `analysis_date: str` as a new required parameter after `json_schema_description`. Update the docstring to document it. Add `analysis_date=analysis_date` to the `template.format()` call.

Updated signature:
```python
def build_prompt(
    template: str,
    filing_id: str,
    filing_date: str | None,
    applicant: str | None,
    filing_type: str | None,
    document_text: str,
    num_documents: int,
    num_missing: int,
    json_schema_description: str,
    analysis_date: str,
) -> str:
```

And in the format call:
```python
return template.format(
    filing_id=filing_id,
    filing_date=filing_date or "Unknown",
    applicant=applicant or "Unknown",
    filing_type=filing_type or "Unknown",
    document_text=document_text,
    num_documents=num_documents,
    num_missing=num_missing,
    json_schema_description=json_schema_description,
    analysis_date=analysis_date,
)
```
  </action>
  <verify>
Run: `cd C:\Users\amelv\Repo\cer_repository_scrapper && uv run python -c "
from cer_scraper.analyzer.prompt import get_json_schema_description, build_prompt
desc = get_json_schema_description()
assert 'regulatory_implications' in desc
assert 'dates' in desc
assert 'sentiment' in desc
assert 'quotes' in desc
assert 'impact' in desc
assert 'temporal_status' in desc
assert 'source_location' in desc
print('Schema description includes all Phase 6 fields')

# Test build_prompt with analysis_date
import inspect
sig = inspect.signature(build_prompt)
assert 'analysis_date' in sig.parameters, 'analysis_date param missing'
# Quick format test with a mini template
test_template = 'Date: {analysis_date}, ID: {filing_id}, Schema: {json_schema_description}, Text: {document_text}, Filed: {filing_date}, App: {applicant}, Type: {filing_type}, Docs: {num_documents}, Missing: {num_missing}'
result = build_prompt(
    template=test_template, filing_id='C123', filing_date=None, applicant=None,
    filing_type=None, document_text='hello', num_documents=1, num_missing=0,
    json_schema_description='schema', analysis_date='2026-02-16'
)
assert '2026-02-16' in result
assert 'C123' in result
print('build_prompt with analysis_date works correctly')
"`
  </verify>
  <done>get_json_schema_description returns a description covering all 10 AnalysisOutput fields (5 Phase 5 + 5 Phase 6). build_prompt accepts and forwards analysis_date to template.format().</done>
</task>

</tasks>

<verification>
1. `uv run python -c "from cer_scraper.analyzer.schemas import AnalysisOutput, RegulatoryImplications, ExtractedDate, SentimentAssessment, RepresentativeQuote, ImpactScore; print('All models importable')"` -- imports succeed
2. Phase 5 backward compatibility: AnalysisOutput.model_validate with only Phase 5 fields succeeds
3. Phase 6 full validation: AnalysisOutput.model_validate with all 10 fields succeeds
4. ImpactScore rejects score=0 and score=6 (constraint ge=1, le=5)
5. build_prompt signature includes analysis_date parameter
6. get_json_schema_description output contains all Phase 6 field names
</verification>

<success_criteria>
- Five new Pydantic models (RegulatoryImplications, ExtractedDate, SentimentAssessment, RepresentativeQuote, ImpactScore) exist in schemas.py
- AnalysisOutput extended with 5 new fields, all having defaults for backward compat
- get_json_schema_description returns Phase 6 field descriptions matching exact field names in schemas
- build_prompt accepts analysis_date and passes it to template.format()
- Existing Phase 5 analysis JSON validates against extended AnalysisOutput without error
</success_criteria>

<output>
After completion, create `.planning/phases/06-deep-analysis-features/06-01-SUMMARY.md`
</output>
