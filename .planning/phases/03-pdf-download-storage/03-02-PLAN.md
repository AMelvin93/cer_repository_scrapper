---
phase: 03-pdf-download-storage
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/cer_scraper/downloader/__init__.py
  - src/cer_scraper/db/state.py
autonomous: true

must_haves:
  truths:
    - "Running the download orchestrator downloads all PDFs for filings with status_downloaded='pending'"
    - "PDFs are saved to data/filings/{YYYY-MM-DD}_Filing-{ID}/documents/doc_001.pdf naming convention"
    - "A filing is marked as downloaded only when ALL its PDFs succeed (all-or-nothing)"
    - "A filing with failed downloads is marked as download_failed and its partial files are cleaned up"
    - "Re-running the pipeline skips filings already marked as downloaded in the database"
    - "One filing's failure does not block others -- the orchestrator continues to the next filing"
    - "The existing rate limiter is reused between PDF downloads"
  artifacts:
    - path: "src/cer_scraper/downloader/__init__.py"
      provides: "Filing-level download orchestrator with all-or-nothing semantics"
      min_lines: 80
    - path: "src/cer_scraper/db/state.py"
      provides: "get_filings_for_download() query function"
      contains: "get_filings_for_download"
  key_links:
    - from: "src/cer_scraper/downloader/__init__.py"
      to: "src/cer_scraper/downloader/service.py"
      via: "download_pdf() call per document"
      pattern: "download_pdf"
    - from: "src/cer_scraper/downloader/__init__.py"
      to: "src/cer_scraper/db/state.py"
      via: "mark_step_complete('downloaded') after all docs succeed"
      pattern: "mark_step_complete.*downloaded"
    - from: "src/cer_scraper/downloader/__init__.py"
      to: "src/cer_scraper/scraper/rate_limiter.py"
      via: "wait_between_requests() between PDF downloads"
      pattern: "wait_between_requests"
    - from: "src/cer_scraper/downloader/__init__.py"
      to: "src/cer_scraper/db/models.py"
      via: "Filing.documents relationship for Document URLs"
      pattern: "filing\\.documents"
---

<objective>
Build the filing-level download orchestrator that iterates over pending filings, downloads all their PDFs using the download service from Plan 01, enforces all-or-nothing semantics, and updates database state.

Purpose: Completes Phase 3 -- the pipeline can now download all PDFs for scraped filings with resilience, skip logic, and per-filing error isolation.
Output: `src/cer_scraper/downloader/__init__.py` with public `download_filings()` API, plus a state query helper.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-pdf-download-storage/03-CONTEXT.md
@.planning/phases/03-pdf-download-storage/03-RESEARCH.md
@.planning/phases/03-pdf-download-storage/03-01-SUMMARY.md
@src/cer_scraper/db/models.py
@src/cer_scraper/db/state.py
@src/cer_scraper/scraper/rate_limiter.py
@src/cer_scraper/downloader/service.py
@src/cer_scraper/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add get_filings_for_download to state store</name>
  <files>
    src/cer_scraper/db/state.py
    src/cer_scraper/db/__init__.py
  </files>
  <action>
    Add a new function `get_filings_for_download()` to `src/cer_scraper/db/state.py`:

    ```python
    def get_filings_for_download(
        session: Session, max_retries: int = 3
    ) -> list[Filing]:
        """Return filings that need PDF downloads.

        A filing needs download if:
            - status_scraped == "success" (scraping completed), AND
            - status_downloaded != "success" (not yet downloaded), AND
            - retry_count < max_retries (not exhausted)

        Eagerly loads the documents relationship so callers can iterate
        documents without additional queries.
        """
    ```

    Implementation:
    - Use `select(Filing).where(...)` with the three conditions above
    - Use `options(selectinload(Filing.documents))` to eagerly load documents (import from `sqlalchemy.orm`)
    - Return `list(session.scalars(stmt).all())`

    Also update `src/cer_scraper/db/__init__.py`:
    - Add `get_filings_for_download` to imports from `.state`
    - Add it to the `__all__` list

    Note: Import `selectinload` from `sqlalchemy.orm` at the top of state.py.
  </action>
  <verify>
    Run: `uv run python -c "from cer_scraper.db import get_filings_for_download; print('import OK')"`
    Expected: Import succeeds without errors.
  </verify>
  <done>get_filings_for_download() returns filings with status_scraped=success, status_downloaded!=success, under retry limit, with eagerly loaded documents.</done>
</task>

<task type="auto">
  <name>Task 2: Create filing-level download orchestrator</name>
  <files>
    src/cer_scraper/downloader/__init__.py
  </files>
  <action>
    Replace the stub `__init__.py` with the full download orchestrator. Must implement:

    1. **`DownloadBatchResult` dataclass**:
       ```python
       @dataclass
       class DownloadBatchResult:
           filings_attempted: int = 0
           filings_succeeded: int = 0
           filings_failed: int = 0
           filings_skipped: int = 0  # already downloaded (shouldn't happen with proper query)
           total_pdfs_downloaded: int = 0
           total_bytes: int = 0
           errors: list[str] = field(default_factory=list)
       ```

    2. **`_build_filing_dir(filing, base_dir)` helper**:
       - Takes a Filing ORM object and base_dir (pathlib.Path)
       - Returns `base_dir / "{YYYY-MM-DD}_Filing-{filing_id}" / "documents"`
       - If `filing.date` is None, use `"unknown-date"` as the date prefix
       - Format date as `filing.date.strftime("%Y-%m-%d")` if present

    3. **`_download_filing(filing, pipeline_settings, scraper_settings, http_client)` helper**:
       - Takes a Filing ORM object (with eagerly loaded documents), PipelineSettings, ScraperSettings, and httpx.Client
       - Builds the filing directory via `_build_filing_dir`
       - Iterates over `filing.documents` sequentially
       - For each document, assigns filename `doc_{NNN}.pdf` (1-indexed, zero-padded to 3 digits: `doc_001.pdf`, `doc_002.pdf`)
       - Calls `download_pdf(doc.document_url, dest_path, settings, http_client)` from service module
       - After each successful download, update the Document record: set `local_path` to str(dest_path), `file_size_bytes` to result.bytes_downloaded, `download_status` to "success"
       - Calls `wait_between_requests(scraper_settings.delay_min_seconds, scraper_settings.delay_max_seconds)` between downloads (import from `cer_scraper.scraper.rate_limiter`)
       - **All-or-nothing logic**: If ANY document fails:
         - Delete the entire filing directory tree (use `shutil.rmtree(filing_dir, ignore_errors=True)`)
         - Reset all Document records for this filing: set `download_status` to "failed", clear `local_path`
         - Return `(False, error_message)`
       - If ALL documents succeed, return `(True, None)`

    4. **`download_filings(session, pipeline_settings, scraper_settings)` public function**:
       - Takes SQLAlchemy session, PipelineSettings, and ScraperSettings
       - Resolve `settings.filings_dir` to absolute path via `(PROJECT_ROOT / settings.filings_dir).resolve()` (import PROJECT_ROOT from config.settings)
       - Call `get_filings_for_download(session, settings.max_retry_count)` to get pending filings
       - If no filings, log info and return empty DownloadBatchResult
       - Create ONE httpx.Client with `timeout=settings.download_timeout_seconds` and reuse for all downloads (use context manager)
       - For each filing:
         - Call `_download_filing(filing, settings, http_client)`
         - On success: call `mark_step_complete(session, filing.filing_id, "downloaded", "success")` and commit Document updates
         - On failure: call `mark_step_complete(session, filing.filing_id, "downloaded", "failed", error=error_msg)`, log error, append to result.errors
         - Each filing's success/failure is independent -- one failure does NOT stop others
         - Commit after each filing (not at the end) to avoid long-running transactions
       - Return DownloadBatchResult with all counts
       - Wrap entire function in try/except -- orchestrator must never crash the pipeline (same pattern as scraper orchestrator)

    5. **Module exports**:
       ```python
       __all__ = ["download_filings", "DownloadBatchResult"]
       ```

    Important implementation details:
    - Use `from __future__ import annotations`
    - Use `logging.getLogger(__name__)` for logging
    - **ScraperSettings required for rate limiter**: `download_filings()` signature MUST accept both `pipeline_settings: PipelineSettings` and `scraper_settings: ScraperSettings` as separate parameters. Import `ScraperSettings` from `cer_scraper.config.settings`. The rate limiter `wait_between_requests()` needs `scraper_settings.delay_min_seconds` and `scraper_settings.delay_max_seconds`. Pass these through to `_download_filing()` as well.
    - Session commits: call `session.commit()` after updating Document records for each filing, and after `mark_step_complete()` (which already commits internally)
  </action>
  <verify>
    Run: `uv run python -c "from cer_scraper.downloader import download_filings, DownloadBatchResult; print('import OK'); r = DownloadBatchResult(); print(r)"`
    Expected: Imports succeed, DownloadBatchResult prints with correct default fields.

    Also verify the folder path logic:
    Run: `uv run python -c "
from pathlib import Path
from cer_scraper.downloader import _build_filing_dir
from unittest.mock import Mock
import datetime
f = Mock()
f.filing_id = '12345'
f.date = datetime.date(2026, 2, 5)
p = _build_filing_dir(f, Path('data/filings'))
print(p)
assert str(p) == 'data/filings/2026-02-05_Filing-12345/documents', f'Got: {p}'
print('path OK')
"`
    Expected: Path matches `data/filings/2026-02-05_Filing-12345/documents`.

    Verify orchestration wiring (key patterns present in __init__.py):
    Run: `uv run python -c "
import pathlib
code = pathlib.Path('src/cer_scraper/downloader/__init__.py').read_text()
checks = ['download_pdf', 'mark_step_complete', 'wait_between_requests', 'shutil.rmtree', 'ScraperSettings', 'get_filings_for_download']
for c in checks:
    assert c in code, f'Missing: {c}'
    print(f'OK: {c}')
print('All orchestration patterns present')
"`
    Expected: All 6 patterns found in the orchestrator code.
  </verify>
  <done>
    download_filings() iterates pending filings, downloads all documents per filing sequentially with rate limiting, enforces all-or-nothing (cleanup on failure), updates Document records and Filing status, and returns DownloadBatchResult. One filing's failure does not block others.
  </done>
</task>

</tasks>

<verification>
1. `get_filings_for_download()` returns only filings with status_scraped=success, status_downloaded!=success, under retry limit
2. `download_filings()` and `DownloadBatchResult` import cleanly from `cer_scraper.downloader`
3. Filing directory path follows `{YYYY-MM-DD}_Filing-{ID}/documents/` convention (with unknown-date fallback)
4. PDF filenames use `doc_001.pdf`, `doc_002.pdf` sequential numbering
5. All-or-nothing: if one document fails, entire filing directory is cleaned up and all documents reset
6. Rate limiter called between downloads (reusing scraper rate limiter)
7. Database state updated correctly: status_downloaded = success/failed, Document.local_path set
8. Orchestrator never crashes -- top-level catch-all returns errors in DownloadBatchResult
</verification>

<success_criteria>
- `uv run python -c "from cer_scraper.downloader import download_filings, DownloadBatchResult"` passes
- `uv run python -c "from cer_scraper.db import get_filings_for_download"` passes
- Folder path helper produces correct `{date}_Filing-{id}/documents/` structure
- Orchestrator handles empty filing list gracefully (returns empty result)
- Code contains `mark_step_complete`, `download_pdf`, `wait_between_requests`, `shutil.rmtree` calls
</success_criteria>

<output>
After completion, create `.planning/phases/03-pdf-download-storage/03-02-SUMMARY.md`
</output>
