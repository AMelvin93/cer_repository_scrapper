---
phase: 03-pdf-download-storage
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cer_scraper/config/settings.py
  - config/pipeline.yaml
  - src/cer_scraper/downloader/__init__.py
  - src/cer_scraper/downloader/service.py
autonomous: true

must_haves:
  truths:
    - "A single PDF URL can be downloaded to a local .pdf file via streaming chunked writes"
    - "Downloads that receive HTML instead of PDF binary are rejected with a warning"
    - "A failed download retries up to 3 times with exponential backoff before giving up"
    - "Partial/incomplete files are deleted on failure (no corrupt PDFs left on disk)"
    - "PDFs above the configured max size are skipped with a warning"
    - "Download config values (filings_dir, max_pdf_size_bytes, chunk_size, timeout) are externalized in pipeline.yaml"
  artifacts:
    - path: "src/cer_scraper/downloader/service.py"
      provides: "Per-PDF streaming download with retry, Content-Type checking, .tmp rename"
      min_lines: 80
    - path: "src/cer_scraper/downloader/__init__.py"
      provides: "Package init stub"
    - path: "src/cer_scraper/config/settings.py"
      provides: "PipelineSettings with download config fields"
      contains: "filings_dir"
    - path: "config/pipeline.yaml"
      provides: "Download config defaults"
      contains: "max_pdf_size_bytes"
  key_links:
    - from: "src/cer_scraper/downloader/service.py"
      to: "httpx streaming"
      via: "httpx.Client.stream('GET', url)"
      pattern: "stream.*GET"
    - from: "src/cer_scraper/downloader/service.py"
      to: "tenacity retry"
      via: "@retry decorator"
      pattern: "@retry"
    - from: "src/cer_scraper/downloader/service.py"
      to: ".tmp rename pattern"
      via: "write to .tmp then rename on success"
      pattern: "\\.tmp"
---

<objective>
Add download configuration fields and build the per-PDF download service with streaming, Content-Type validation, retry logic, and cleanup on failure.

Purpose: Provides the core download capability that the filing-level orchestrator (Plan 02) will call for each document.
Output: `src/cer_scraper/downloader/service.py` with a `download_pdf()` function, plus config extensions.
</objective>

<execution_context>
@C:\Users\amelv\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\amelv\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-pdf-download-storage/03-CONTEXT.md
@.planning/phases/03-pdf-download-storage/03-RESEARCH.md
@src/cer_scraper/config/settings.py
@config/pipeline.yaml
@src/cer_scraper/scraper/rate_limiter.py
@src/cer_scraper/db/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add download configuration fields to PipelineSettings</name>
  <files>
    src/cer_scraper/config/settings.py
    config/pipeline.yaml
  </files>
  <action>
    Add four new fields to PipelineSettings in settings.py:

    - `filings_dir: str = "data/filings"` -- root directory for downloaded filing folders
    - `max_pdf_size_bytes: int = 104_857_600` -- 100MB default max file size
    - `download_chunk_size: int = 65_536` -- 64KB chunks for streaming writes
    - `download_timeout_seconds: int = 120` -- per-PDF download timeout

    Add matching defaults to config/pipeline.yaml under a `# Phase 3: download settings` comment block:

    ```yaml
    # Phase 3: download settings
    filings_dir: "data/filings"
    max_pdf_size_bytes: 104857600  # 100MB
    download_chunk_size: 65536  # 64KB
    download_timeout_seconds: 120
    ```

    These follow the established pattern of all other config fields -- YAML defaults + env var overrides via PIPELINE_ prefix.
  </action>
  <verify>
    Run: `uv run python -c "from cer_scraper.config.settings import PipelineSettings; s = PipelineSettings(); print(s.filings_dir, s.max_pdf_size_bytes, s.download_chunk_size, s.download_timeout_seconds)"`
    Expected output: `data/filings 104857600 65536 120`
  </verify>
  <done>PipelineSettings loads all four download config fields from pipeline.yaml with correct defaults.</done>
</task>

<task type="auto">
  <name>Task 2: Create PDF download service with streaming, retry, and cleanup</name>
  <files>
    src/cer_scraper/downloader/__init__.py
    src/cer_scraper/downloader/service.py
  </files>
  <action>
    Create the `src/cer_scraper/downloader/` package.

    **`__init__.py`**: Empty stub with docstring. Public API will be added in Plan 02.

    **`service.py`**: Single-PDF download service. Must implement:

    1. **`download_pdf(url, dest_path, settings, http_client)`** function:
       - `url`: The document URL (may be direct PDF link OR `/Item/View/{ID}` viewer URL)
       - `dest_path`: pathlib.Path where the final PDF should be saved
       - `settings`: PipelineSettings (for max_pdf_size_bytes, download_chunk_size, download_timeout_seconds)
       - `http_client`: httpx.Client instance (caller manages lifecycle)
       - Returns a `DownloadResult` dataclass with fields: `success: bool`, `bytes_downloaded: int`, `error: str | None`

    2. **Content-Type checking**:
       - Use `http_client.stream("GET", url, timeout=settings.download_timeout_seconds, follow_redirects=True)`
       - Check the response `Content-Type` header. If it contains `text/html` (not `application/pdf` or `application/octet-stream`), this is a REGDOCS viewer page, not a direct download. Close the stream, return failure with error "Response is HTML, not a PDF binary".
       - If Content-Type is missing or ambiguous, proceed with download (don't reject).

    3. **File size checking**:
       - If `Content-Length` header is present and exceeds `settings.max_pdf_size_bytes`, skip download immediately. Return failure with error "PDF exceeds max size limit ({size} bytes > {max} bytes)".
       - Also track actual bytes during streaming. If actual bytes exceed max during download, stop writing, delete temp file, return failure.

    4. **Streaming write with .tmp rename pattern**:
       - Create parent directories with `dest_path.parent.mkdir(parents=True, exist_ok=True)`
       - Write to `dest_path.with_suffix('.pdf.tmp')` (temp file)
       - Stream response body in chunks of `settings.download_chunk_size` bytes
       - On success: rename `.pdf.tmp` to `.pdf` (atomic on same filesystem)
       - On ANY failure: delete the `.tmp` file if it exists (use try/finally)

    5. **tenacity retry decorator**:
       - Wrap the core download logic in a private `_download_with_retry()` function decorated with `@retry`:
         ```python
         @retry(
             stop=stop_after_attempt(3),
             wait=wait_exponential(multiplier=1, min=2, max=30),
             retry=retry_if_exception_type((httpx.HTTPStatusError, httpx.TransportError)),
             before_sleep=before_sleep_log(logger, logging.WARNING),
             reraise=True,
         )
         ```
       - The public `download_pdf()` catches the final exception after retries exhausted and returns a DownloadResult with success=False.

    6. **DownloadResult dataclass**:
       ```python
       @dataclass
       class DownloadResult:
           success: bool
           bytes_downloaded: int = 0
           error: str | None = None
       ```

    7. **Logging**: Use `logging.getLogger(__name__)`. Log at INFO level when starting download, DEBUG for chunk progress, WARNING for retries, ERROR for final failures.

    Important implementation notes:
    - Use `from __future__ import annotations` at top
    - Import tenacity components: `retry, stop_after_attempt, wait_exponential, retry_if_exception_type, before_sleep_log`
    - The function should raise `httpx.HTTPStatusError` on non-2xx responses (call `response.raise_for_status()`) to trigger retry
    - Do NOT manage httpx.Client lifecycle in this module -- caller creates and passes it
  </action>
  <verify>
    Run: `uv run python -c "from cer_scraper.downloader.service import download_pdf, DownloadResult; print('import OK'); r = DownloadResult(success=True, bytes_downloaded=1024); print(r)"`
    Expected: Imports succeed, DownloadResult prints with correct fields.
  </verify>
  <done>
    download_pdf() function exists with streaming writes, .tmp rename, Content-Type rejection, file size limits, tenacity retry (3 attempts, exponential backoff), and temp file cleanup on failure. DownloadResult dataclass carries success/bytes/error.
  </done>
</task>

</tasks>

<verification>
1. PipelineSettings loads 4 new download config fields with correct defaults
2. `download_pdf` and `DownloadResult` import cleanly from `cer_scraper.downloader.service`
3. The download service uses tenacity retry decorator with 3 attempts and exponential backoff
4. The .tmp rename pattern is implemented (write to .tmp, rename on success, delete on failure)
5. Content-Type check rejects HTML responses from REGDOCS viewer URLs
6. File size limit enforced both from Content-Length header and during streaming
</verification>

<success_criteria>
- `uv run python -c "from cer_scraper.config.settings import PipelineSettings; s = PipelineSettings(); assert s.filings_dir == 'data/filings'; assert s.max_pdf_size_bytes == 104857600"` passes
- `uv run python -c "from cer_scraper.downloader.service import download_pdf, DownloadResult"` passes
- service.py contains `@retry` decorator, `.tmp` suffix handling, Content-Type check, and size limit logic
</success_criteria>

<output>
After completion, create `.planning/phases/03-pdf-download-storage/03-01-SUMMARY.md`
</output>
